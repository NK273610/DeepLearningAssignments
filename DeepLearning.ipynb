{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IQaTiatZGmoI"
   },
   "source": [
    "# <center>Assignment 3</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2b-ZK9tuGmoK"
   },
   "source": [
    "This assignment is based on embeddings and CNNs. You can choose to code in Python2 or Python3. All the imports made in this notebook are as below; if these imports work, you are (mostly) set to complete the assignment. You will learn the following:\n",
    "* Making use of embeddings in Tensorflow\n",
    "* Coding CNNs in TF\n",
    "* Intuitions behind working of CNN\n",
    "* Intuitions behind embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Zjqi3WjaGmoL"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function,division\n",
    "import random \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 103,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10437,
     "status": "ok",
     "timestamp": 1529106135229,
     "user": {
      "displayName": "Nikhil Dhirmalani",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "117732879546167846741"
     },
     "user_tz": 180
    },
    "id": "VsVSs1HS0qDi",
    "outputId": "f4069e25-c184-4921-b7f6-cdcae17b9db4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-4aed577c-6c83-4f6f-b5f9-efdca516aa39\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-4aed577c-6c83-4f6f-b5f9-efdca516aa39\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving class_neg.txt to class_neg.txt\n",
      "Saving class_pos.txt to class_pos.txt\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "In8BxTz4GmoO"
   },
   "source": [
    "## Quick Review questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5WuBRadWGmoO"
   },
   "source": [
    "* If the input volume has dimensions 10 x 10 x 32 (Height x Width x Channels), how many weights will be there in a filter that considers an area of 5 x 5?\n",
    "\n",
    "<b>Answer:</b> 800\n",
    "* If input volume has dimensions 10 x 10 x 32 and after convolution we get an output volume of 8 x 8 x 64, how many filters were used? \n",
    "\n",
    "<b>Answer:</b> 64\n",
    "* What is inverted-dropout? Why is it done? \n",
    "\n",
    "<b>Answer:</b> In dropout you try to make activations of some neurons zero with probablity. The inverted drop out technique is dividing your activations vector with keep probablity, this is done in order to ensure that the expected value of that activation vector remains the same. THis also ensures that you don't have to perform a scaling at test time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vWzfE86tusct"
   },
   "source": [
    "## Sentiment Classification - dataset analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "atbvZSXlGmoQ"
   },
   "source": [
    "We will use movie review dataset taken from http://www.cs.cornell.edu/people/pabo/movie-review-data/. The exact dataset we will use is the Sentence-polarity dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 343,
     "status": "ok",
     "timestamp": 1529106141167,
     "user": {
      "displayName": "Nikhil Dhirmalani",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "117732879546167846741"
     },
     "user_tz": 180
    },
    "id": "6l_qzOlhGmoR",
    "outputId": "06333c02-cb64-47fa-d215-066b4786aa7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews of class_neg = 5331\n",
      "\tMax number of tokens in a sentence = 56\n",
      "\tMin number of tokens in a sentence = 1\n",
      "Number of reviews of class_pos = 5331\n",
      "\tMax number of tokens in a sentence = 59\n",
      "\tMin number of tokens in a sentence = 2\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for file_,label in zip([\"class_neg.txt\",\"class_pos.txt\"],[0,1]):\n",
    "    lines = open(file_).readlines()\n",
    "    lines = list(map(lambda x:x.strip().replace(\"-\",\" \").split(),lines))\n",
    "    for line in lines:\n",
    "        data.append([line,label])\n",
    "    print(\"Number of reviews of {} = {}\".format(file_[:-4],len(lines)))\n",
    "    print(\"\\tMax number of tokens in a sentence = {}\".format(max(map(lambda x:len(x),lines))))\n",
    "    print(\"\\tMin number of tokens in a sentence = {}\".format(min(map(lambda x:len(x),lines))))\n",
    "random.Random(5).shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sKqVN0XTGmoV"
   },
   "source": [
    "Observe that the lengths of sentences are different. In case, we need to vectorize the operations, we need all sentences to be of equal length. Therefore, we will pad all sentences to be of equal length and substitute the padded parts of sentence with zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 248,
     "status": "ok",
     "timestamp": 1529106142527,
     "user": {
      "displayName": "Nikhil Dhirmalani",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "117732879546167846741"
     },
     "user_tz": 180
    },
    "id": "UJSIi3EOGmoV",
    "outputId": "ba47c18f-cc5e-4629-d1d1-04f54b8db8ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it appears that something has been lost in the translation to the screen .\n"
     ]
    }
   ],
   "source": [
    "# See some randomly sampled sentences\n",
    "print(\" \".join(data[random.randint(0,len(data))][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x70n4lA7GmoY"
   },
   "source": [
    "We will work with the sentence as given and not remove any stop-words or punctuation marks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 417,
     "status": "ok",
     "timestamp": 1529106143943,
     "user": {
      "displayName": "Nikhil Dhirmalani",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "117732879546167846741"
     },
     "user_tz": 180
    },
    "id": "959vht4CGmoZ",
    "outputId": "cde5a318-4b01-4488-cade-68a28c600c03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words :  19757\n",
      "train\n",
      "\tNumber of positive examples :  4288\n",
      "\tNumber of negative examples :  4241\n",
      "test\n",
      "\tNumber of positive examples :  1043\n",
      "\tNumber of negative examples :  1090\n"
     ]
    }
   ],
   "source": [
    "sents = map(lambda x:x[0],data) # all sentences\n",
    "all_words = set()\n",
    "for sent in sents:\n",
    "    all_words |= set(sent)\n",
    "all_words = sorted(list(all_words))\n",
    "vocab = {all_words[i]:i for i in range(len(all_words))}\n",
    "print(\"Number of words : \",len(vocab))\n",
    "train = data[:int(0.8*len(data))]\n",
    "test = data[int(0.8*len(data)):]\n",
    "train_data = []\n",
    "train_targets = []\n",
    "test_data = []\n",
    "test_targets = []\n",
    "for list_all,list_data,list_target,label_list in zip([train,test],[train_data,test_data],[train_targets,test_targets],[\"train\",\"test\"]):\n",
    "    for datum,label in list_all:\n",
    "        list_data.append([vocab[w] for w in datum])\n",
    "        list_target.append([label])\n",
    "    print(label_list)\n",
    "    print(\"\\tNumber of positive examples : \",list_target.count([1]))\n",
    "    print(\"\\tNumber of negative examples : \",list_target.count([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 249,
     "status": "ok",
     "timestamp": 1529106144688,
     "user": {
      "displayName": "Nikhil Dhirmalani",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "117732879546167846741"
     },
     "user_tz": 180
    },
    "id": "L0r2BBJND-aK",
    "outputId": "5afefff8-9831-4a6f-c78a-716c9cb7740b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [1], [1], [0], [0], [0], [1], [1], [1], [1], [1], [1], [0], [1], [1], [0], [1], [1], [0], [0], [1], [0], [1], [1], [1], [0], [0], [1], [0], [1], [1], [0], [1], [1], [0], [1], [0], [1], [0], [1], [0], [0], [1], [0], [1], [0], [1], [1], [1], [0], [0], [0], [1], [0], [1], [1], [1], [1], [1], [1], [1], [1], [0], [0], [0], [1], [0], [0], [0], [0], [1], [1], [0], [0], [1], [1], [0], [0], [1], [0], [1], [0], [1], [0], [0], [0], [0], [1], [0], [1], [1], [0], [1], [1], [1], [1], [0], [0], [1], [1], [1], [0], [0], [1], [0], [0], [1], [0], [0], [1], [1], [0], [1], [1], [1], [0], [0], [1], [1], [1], [1], [0], [0], [0], [1], [1], [1], [0], [0], [0], [0], [1], [0], [1], [0], [0], [0], [1], [1], [1], [1], [0], [0], [1], [0], [0], [1], [0], [1], [0], [0], [0], [1], [1], [1], [0], [0], [1], [0], [1], [1], [0], [1], [1], [0], [0], [0], [1], [0], [1], [0], [1], [0], [0], [0], [0], [0], [0], [0], [1], [1], [1], [0], [0], [0], [1], [0], [1], [0], [0], [0], [1], [0], [1], [0], [1], [1], [0], [0], [1], [0], [1], [1], [0], [1], [0], [0], [1], [0], [1], [0], [1], [1], [1], [1], [0], [0], [1], [1], [0], [1], [0], [0], [1], [1], [0], [1], [0], [1], [0], [0], [1], [1], [0], [1], [0], [0], [0], [0], [1], [1], [0], [1], [1], [1], [0], [1], [0], [0], [0], [1], [1], [1], [1], [0], [1], [0], [1], [0], [1], [0], [1], [1], [0], [1], [0], [0], [1], [1], [0], [0], [0], [1], [1], [0], [0], [0], [0], [1], [1], [0], [1], [1], [1], [0], [1], [0], [1], [0], [0], [0], [1], [0], [1], [0], [1], [0], [0], [1], [1], [1], [0], [0], [0], [0], [0], [1], [1], [1], [1], [0], [1], [1], [0], [0], [0], [0], [0], [0], [0], [1], [1], [1], [0], [0], [1], [1], [0], [1], [1], [1], [1], [0], [1], [0], [1], [0], [0], [0], [1], [1], [1], [0], [1], [0], [1], [0], [0], [1], [1], [1], [1], [1], [1], [0], [0], [1], [0], [1], [0], [0], [1], [1], [0], [1], [0], [1], [1], [0], [1], [1], [1], [0], [0], [0], [1], [1], [1], [1], [0], [0], [0], [0], [1], [1], [1], [0], [0], [1], [0], [0], [0], [0], [0], [0], [1], [1], [0], [1], [0], [1], [1], [0], [1], [1], [1], [1], [0], [0], [1], [1], [1], [1], [0], [1], [1], [0], [0], [0], [1], [0], [1], [1], [1], [1], [0], [0], [0], [0], [1], [0], [1], [0], [0], [1], [0], [0], [1], [0], [0], [1], [1], [1], [1], [0], [0], [0], [1], [1], [0], [0], [1], [1], [1], [1], [1], [0], [0], [1], [0], [0], [1], [1], [1], [0], [0], [0], [0], [1], [0], [1], [0], [0], [1], [1], [1], [0], [0], [0], [1], [0], [1], [0], [1], [0], [0], [1], [1], [0], [1], [0], [0], [1], [1], [0], [0], [1], [0], [0], [0], [0], [1], [1], [0], [0], [1], [1], [0], [0], [0], [1], [1], [1], [1], [1], [1], [0], [0], [1], [1], [0], [0], [0], [1], [0], [1], [1], [0], [0], [1], [0], [1], [0], [0], [1], [1], [0], [1], [0], [1], [0], [0], [0], [0], [0], [1], [0], [0], [0], [0], [1], [1], [1], [0], [1], [1], [1], [1], [0], [0], [0], [1], [0], [1], [0], [0], [0], [1], [1], [0], [0], [1], [1], [0], [1], [0], [0], [1], [1], [1], [1], [0], [0], [1], [1], [0], [1], [1], [1], [1], [0], [0], [1], [1], [0], [0], [0], [0], [1], [1], [0], [1], [0], [0], [1], [0], [1], [1], [1], [1], [1], [1], [1], [0], [0], [0], [0], [0], [1], [0], [0], [0], [1], [0], [1], [1], [1], [0], [1], [1], [0], [1], [1], [0], [1], [0], [0], [0], [1], [1], [0], [1], [1], [0], [0], [0], [1], [0], [1], [1], [0], [0], [0], [0], [0], [0], [1], [0], [1], [1], [1], [0], [1], [0], [1], [0], [1], [1], [0], [0], [1], [1], [0], [1], [0], [1], [1], [0], [1], [0], [0], [0], [1], [1], [0], [0], [1], [0], [1], [1], [1], [1], [0], [0], [0], [1], [0], [0], [0], [0], [0], [1], [0], [1], [0], [0], [0], [1], [0], [1], [1], [0], [0], [0], [1], [0], [0], [0], [0], [1], [0], [1], [0], [0], [0], [1], [1], [0], [1], [1], [0], [0], [1], [0], [0], [1], [0], [1], [0], [0], [0], [1], [0], [0], [0], [1], [1], [0], [1], [1], [1], [1], [0], [1], [1], [0], [0], [0], [1], [0], [1], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [1], [0], [1], [1], [0], [1], [1], [0], [0], [1], [1], [0], [0], [1], [0], [0], [0], [0], [1], [1], [0], [0], [1], [0], [0], [1], [1], [1], [0], [0], [1], [0], [1], [0], [0], [0], [1], [0], [0], [0], [1], [1], [1], [0], [0], [0], [0], [1], [1], [0], [0], [1], [1], [0], [0], [0], [1], [1], [1], [0], [1], [1], [1], [1], [0], [1], [0], [0], [1], [1], [0], [0], [1], [1], [1], [0], [1], [1], [1], [1], [0], [1], [0], [0], [1], [1], [1], [0], [1], [1], [0], [1], [1], [1], [0], [0], [1], [1], [0], [0], [1], [1], [1], [0], [1], [1], [0], [1], [0], [0], [0], [1], [1], [1], [1], [1], [0], [1], [1], [0], [0], [1], [0], [1], [1], [1], [0], [0], [0], [0], [1], [0], [1], [0], [1], [0], [0], [1], [1], [0], [0], [1], [1], [1], [1], [1], [1], [1], [0], [1], [0], [1], [0], [1], [0], [0], [0], [0], [0], [1], [1], [1], [1], [1], [1], [0], [0], [0], [1], [0], [1], [0], [1], [1], [1], [0], [1], [0], [1], [1], [0], [1], [1], [0], [1], [1], [0], [1], [0], [0], [0], [1], [1], [1], [0], [1], [0], [1], [1], [1], [0], [1], [1], [1], [1], [1], [1], [1], [0], [1], [0], [0], [1], [1], [1], [0], [0], [1], [1], [1], [0], [0], [0], [1], [1], [1], [0], [0], [1], [1], [1], [0], [0], [0], [1], [0], [1], [0], [0], [1], [0], [0], [0], [1], [1], [1], [0], [0], [0], [1], [1], [0], [1], [1], [1], [0], [0], [0], [0], [1], [1], [1], [0], [0], [0], [1], [1], [1], [0], [0], [1], [0], [1], [1], [1], [0], [0], [1], [0], [0], [0], [0], [1], [1], [1], [0], [1], [0], [1], [0], [1], [1], [1], [1], [1], [1], [0], [1], [0], [1], [0], [0], [0], [1], [1], [1], [1], [0], [1], [1], [1], [1], [0], [1], [0], [1], [0], [0], [0], [1], [1], [1], [1], [0], [0], [1], [1], [0], [1], [1], [1], [1], [1], [0], [1], [1], [0], [1], [0], [0], [1], [0], [0], [1], [0], [1], [0], [1], [0], [1], [1], [1], [0], [0], [0], [1], [1], [1], [1], [1], [0], [1], [1], [1], [0], [1], [0], [1], [0], [1], [0], [0], [0], [0], [1], [0], [0], [0], [1], [1], [0], [0], [1], [1], [1], [0], [1], [0], [1], [1], [0], [0], [0], [0], [0], [1], [1], [0], [1], [1], [0], [0], [1], [0], [1], [0], [0], [1], [1], [1], [1], [1], [1], [0], [1], [1], [0], [1], [1], [1], [1], [1], [0], [0], [0], [1], [1], [1], [1], [1], [1], [0], [1], [1], [1], [0], [1], [1], [1], [0], [0], [0], [1], [1], [1], [0], [0], [1], [1], [1], [0], [0], [0], [0], [1], [0], [0], [0], [1], [0], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [0], [0], [1], [0], [0], [1], [1], [1], [0], [1], [0], [1], [1], [1], [1], [0], [1], [1], [0], [1], [1], [1], [0], [0], [0], [0], [1], [0], [1], [1], [0], [1], [1], [0], [0], [0], [0], [0], [1], [0], [0], [0], [0], [0], [0], [0], [1], [1], [0], [1], [1], [0], [0], [0], [1], [0], [0], [0], [1], [0], [1], [0], [0], [0], [0], [1], [0], [0], [1], [1], [0], [0], [1], [1], [1], [0], [0], [1], [0], [0], [0], [0], [0], [1], [1], [1], [0], [0], [1], [1], [0], [0], [1], [1], [0], [1], [0], [1], [1], [0], [1], [1], [1], [1], [0], [1], [1], [1], [0], [1], [0], [1], [0], [0], [0], [0], [0], [0], [1], [1], [0], [1], [0], [1], [0], [0], [0], [1], [0], [1], [0], [0], [1], [0], [1], [1], [0], [0], [0], [1], [1], [1], [0], [1], [0], [1], [1], [0], [0], [0], [1], [0], [1], [1], [0], [0], [1], [0], [1], [1], [0], [1], [0], [1], [1], [0], [1], [1], [0], [0], [1], [1], [1], [0], [1], [1], [0], [1], [1], [1], [0], [1], [0], [0], [1], [0], [1], [0], [0], [1], [1], [0], [0], [0], [0], [0], [1], [1], [0], [1], [0], [1], [1], [0], [1], [1], [0], [0], [0], [0], [0], [1], [1], [0], [1], [1], [1], [0], [0], [1], [0], [0], [1], [1], [1], [1], [0], [1], [0], [0], [0], [0], [0], [1], [1], [0], [0], [0], [0], [0], [1], [1], [1], [1], [0], [1], [0], [0], [1], [1], [0], [0], [1], [0], [0], [0], [0], [1], [1], [0], [1], [1], [0], [0], [1], [1], [0], [0], [1], [0], [0], [1], [1], [0], [0], [0], [1], [1], [1], [1], [0], [1], [1], [0], [1], [0], [1], [1], [0], [1], [1], [1], [1], [0], [1], [0], [1], [0], [1], [1], [0], [0], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [0], [1], [1], [0], [0], [0], [1], [1], [1], [0], [1], [1], [1], [1], [1], [1], [0], [1], [1], [0], [1], [0], [1], [0], [0], [0], [1], [1], [1], [1], [1], [0], [1], [0], [1], [1], [1], [1], [0], [1], [1], [1], [0], [1], [1], [0], [0], [0], [1], [1], [1], [0], [1], [1], [0], [1], [1], [0], [0], [0], [0], [0], [1], [0], [1], [0], [1], [1], [0], [0], [0], [0], [0], [0], [0], [0], [0], [1], [1], [1], [0], [1], [1], [1], [0], [0], [0], [0], [1], [0], [1], [1], [0], [1], [1], [0], [1], [0], [1], [0], [1], [1], [1], [0], [1], [0], [0], [0], [0], [1], [1], [1], [1], [0], [1], [0], [0], [0], [0], [0], [0], [1], [0], [1], [1], [0], [0], [1], [0], [0], [1], [0], [1], [1], [1], [1], [1], [1], [0], [0], [1], [1], [0], [1], [1], [1], [1], [1], [1], [0], [1], [0], [0], [1], [1], [0], [1], [0], [1], [0], [1], [1], [0], [1], [0], [0], [1], [1], [0], [0], [0], [1], [0], [0], [1], [1], [0], [0], [1], [0], [0], [0], [0], [1], [0], [0], [0], [1], [0], [0], [0], [0], [1], [0], [1], [0], [1], [0], [0], [0], [0], [1], [1], [1], [0], [1], [1], [0], [0], [1], [0], [1], [1], [1], [0], [1], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [1], [1], [0], [1], [1], [1], [1], [0], [1], [1], [1], [0], [0], [0], [0], [0], [0], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [0], [0], [1], [0], [1], [0], [0], [1], [1], [0], [0], [0], [1], [1], [1], [1], [1], [0], [0], [0], [0], [1], [1], [1], [1], [1], [0], [1], [0], [0], [1], [0], [1], [0], [0], [0], [1], [0], [1], [0], [0], [1], [1], [0], [0], [1], [0], [1], [1], [1], [0], [0], [0], [1], [1], [1], [0], [1], [0], [1], [1], [1], [0], [1], [0], [1], [1], [0], [0], [0], [0], [1], [0], [0], [0], [1], [0], [0], [1], [0], [0], [0], [0], [1], [0], [1], [1], [0], [0], [1], [1], [1], [1], [0], [1], [1], [0], [0], [1], [1], [1], [1], [1], [0], [0], [0], [1], [1], [1], [1], [1], [0], [1], [0], [1], [0], [1], [1], [0], [1], [0], [0], [0], [0], [1], [1], [1], [0], [1], [1], [0], [0], [1], [0], [0], [0], [1], [1], [0], [1], [1], [0], [0], [1], [0], [0], [0], [0], [0], [1], [1], [1], [1], [1], [1], [0], [0], [1], [1], [0], [0], [1], [0], [1], [1], [1], [1], [0], [0], [1], [1], [1], [1], [1], [1], [0], [0], [1], [0], [0], [0], [0], [0], [0], [1], [1], [1], [0], [1], [1], [0], [1], [1], [1], [0], [0], [0], [1], [1], [1], [0], [1], [1], [0], [1], [1], [1], [0], [1], [0], [0], [1], [1], [0], [0], [0], [1], [0], [1], [1], [1], [0], [1], [0], [1], [1], [0], [0], [0], [0], [1], [1], [1], [1], [0], [0], [0], [1], [1], [1], [0], [0], [1], [1], [1], [0], [1], [0], [0], [0], [1], [1], [1], [0], [1], [1], [0], [1], [0], [0], [1], [0], [1], [0], [1], [1], [0], [1], [1], [1], [1], [1], [0], [0], [1], [0], [0], [1], [1], [0], [0], [0], [0], [1], [0], [0], [1], [0], [0], [1], [0], [0], [1], [0], [0], [1], [0], [0], [0], [0], [0], [1], [1], [0], [1], [1], [1], [0], [0], [0], [1], [0], [0], [1], [0], [0], [0], [1], [1], [0], [1], [1], [1], [1], [0], [1], [1], [1], [0], [0], [1], [0], [0], [0], [0], [1], [0], [1], [1], [1], [1], [1], [1], [0], [1], [1], [0], [1], [0], [0], [0], [0], [1], [0], [1], [0], [1], [0], [1], [1], [0], [0], [1], [1], [0], [1], [0], [0], [1], [1], [1], [0], [0], [0], [0], [0], [1], [0], [0], [0], [1], [0], [0], [1], [1], [1], [1], [1], [1], [0], [1], [0], [0], [0], [0], [0], [0], [1], [1], [1], [0], [1], [0], [1], [0], [1], [1], [1], [1], [0], [1], [0], [1], [1], [0], [1], [0], [1], [0], [1], [0], [0], [0], [1], [0], [1], [1], [1], [1], [1], [1], [1], [0], [0], [1], [0], [1], [0], [1], [0], [0], [0], [1], [1], [0], [1], [1], [1], [0], [1], [1], [0], [1], [0], [1], [1], [1], [1], [0], [0], [1], [1], [0], [0], [1], [0], [1], [1], [1], [0], [1], [1], [1], [0], [0], [0], [0], [0], [1], [0], [0], [1], [1], [1], [0], [0], [1], [1], [0], [0], [1], [0], [0], [1], [1], [0], [0], [0], [1], [0], [0], [1], [0], [0], [1], [0], [1], [1], [1], [1], [1], [0], [0], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [0], [1], [1], [1], [0], [1], [0], [1], [1], [1], [0], [1], [0], [0], [1], [1], [0], [0], [1], [0], [1], [0], [1], [1], [0], [0], [1], [0], [0], [0], [1], [1], [0], [0], [1], [0], [1], [0], [0], [1], [0], [1], [1], [1], [0], [0], [1], [1], [1], [1], [1], [1], [0], [0], [1], [0], [0], [1], [0], [0], [1], [1], [1], [1], [0], [0], [1], [1], [0], [1], [0], [0], [1], [1], [0], [1], [1], [0], [0], [1], [1], [1], [0], [1], [1], [1], [1], [0], [0], [0], [1], [0], [0], [0], [0], [0], [0], [1], [1], [0], [1], [0], [1], [0], [1], [1], [0], [1], [1], [0], [1], [1], [0], [1], [1], [1], [0], [0], [1], [0], [1], [0], [1], [0], [1], [1], [0], [0], [1], [1], [1], [0], [1], [1], [1], [1], [1], [0], [0], [1], [0], [0], [0], [1], [1], [0], [1], [1], [1], [1], [1], [1], [1], [1], [0], [1], [1], [1], [0], [0], [0], [1], [0], [0], [1], [0], [0], [1], [0], [1], [0], [0], [1], [0], [1], [0], [1], [1], [1], [1], [0], [1], [0], [1], [0], [0], [1], [0], [0], [1], [0], [1], [0], [1], [1], [1], [1], [0], [1], [1], [0], [1], [0], [0], [0], [1], [0], [0], [1], [1], [1], [0], [0], [1], [1], [0], [1], [0], [0], [0], [0], [1], [0], [1], [0], [1], [1], [0], [0], [1], [0], [1], [0], [1], [0], [0], [1], [1], [1], [1], [0], [1], [0], [1], [0], [0], [1], [0], [1], [1], [0], [1], [0], [1], [0], [1], [0], [1], [1], [0], [0], [1], [1], [0], [1], [0], [0], [1], [1], [0], [1], [1], [0], [0], [1], [0], [1], [1], [1], [0], [1], [0], [1], [0], [1], [1], [1], [0], [0], [1], [0], [1], [0], [1], [0], [1], [1], [0], [0], [0], [0], [0], [0], [0], [1], [0], [0], [0], [0], [1], [1], [0], [0], [0], [0], [0], [1], [0], [1], [1], [0], [0], [1], [1], [0], [1], [0], [0], [0], [1], [0], [0], [1], [0], [0], [0], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [1], [0], [1], [0], [0], [1], [0], [1], [0], [1], [1], [1], [0], [0], [1], [1], [1], [0], [1], [0], [0], [1], [1], [1], [1], [1], [0], [1], [0], [0], [1], [1], [1], [0], [0], [0], [1], [1], [1], [1], [1], [1], [1], [0], [0], [1], [0], [0], [0], [1], [0], [0], [0], [1], [1], [1], [0], [0], [1], [1], [0], [1], [0], [1], [0], [1], [1], [1], [1], [0], [1], [1], [1], [1], [1], [1], [0], [1], [1], [0], [1], [1], [0], [1], [1], [0], [1], [0], [0], [0], [1], [0], [0], [0], [0], [1], [0], [0], [0], [0], [0], [0], [1], [1], [0], [0], [1], [1], [1], [0], [0], [1], [1], [0], [1], [1], [0], [1], [0], [1], [0], [1], [1], [1], [1], [0], [0], [1], [1], [0], [0], [0], [1], [1], [0], [0], [1], [1], [1], [1], [0], [0], [1], [0], [0], [1], [1], [1], [1], [0], [1], [0], [0], [0], [0], [1], [0], [1], [0], [1], [1], [1], [0], [1], [0], [0], [0], [1], [1], [1], [0], [0], [1], [1], [1], [1], [0], [0], [1], [1], [0], [0], [1], [0], [0], [0], [0], [1], [0], [1], [1], [0], [1], [0], [1], [1], [1], [0], [1], [1], [0], [0], [1], [0], [0], [1], [1], [0], [0], [0], [0], [1], [1], [0], [0], [0], [1], [1], [1], [0], [1], [1], [0], [1], [0], [0], [1], [0], [0], [1], [1], [1], [1], [0], [1], [0], [0], [0], [1], [0], [1], [1], [1], [0], [1], [1], [0], [0], [0], [1], [1], [0], [1], [1], [1], [0], [0], [0], [0], [1], [0], [1], [1], [1], [1], [0], [0], [0], [0], [1], [1], [1], [0], [0], [0], [1], [1], [1], [0], [0], [1], [1], [0], [1], [0], [0], [1], [1], [1], [0], [0], [0], [1], [0], [1], [0], [0], [1], [0], [0], [0], [1], [0], [1], [1], [1], [1], [0], [0], [1], [0], [0], [0], [1], [0], [1], [0], [1], [1], [0], [1], [1], [1], [0], [0], [0], [1], [1], [1], [0], [1], [0], [1], [1], [0], [1], [1], [0], [1], [0], [0], [1], [1], [1], [1], [1], [0], [1], [0], [1], [0], [0], [0], [1], [0], [1], [0], [0], [0], [0], [1], [1], [1], [0], [0], [1], [0], [1], [1], [1], [1], [0], [1], [1], [1], [0], [1], [0], [1], [0], [1], [0], [0], [1], [1], [0], [0], [1], [0], [1], [1], [1], [0], [0], [0], [1], [0], [0], [0], [1], [0], [0], [0], [1], [1], [0], [0], [0], [1], [1], [0], [1], [1], [1], [1], [0], [0], [1], [1], [1], [0], [0], [0], [0], [1], [1], [1], [1], [0], [0], [1], [0], [1], [1], [0], [1], [1], [0], [0], [0], [1], [0], [1], [1], [0], [1], [1], [1], [0], [1], [1], [0], [0], [1], [0], [0], [0], [1], [1], [1], [1], [0], [0], [1], [1], [0], [0], [1], [1], [0], [1], [0], [0], [0], [0], [1], [1], [0], [0], [1], [0], [1], [0], [0], [1], [1], [1], [1], [0], [1], [1], [0], [0], [0], [1], [1], [1], [1], [0], [0], [1], [1], [0], [0], [1], [1], [1], [1], [1], [1], [1], [1], [0], [1], [0], [0], [1], [0], [1], [1], [0], [1], [0], [0], [1], [1], [0], [0], [0], [0], [1], [1], [0], [1], [1], [0], [0], [0], [1], [0], [1], [0], [1], [0], [0], [1], [1], [0], [0], [0], [0], [0], [1], [1], [1], [1], [0], [0], [0], [1], [1], [0], [1], [1], [1], [1], [1], [1], [0], [1], [1], [0], [1], [0], [1], [1], [1], [1], [0], [1], [0], [0], [1], [0], [1], [0], [1], [1], [1], [0], [1], [1], [0], [0], [1], [0], [0], [0], [1], [1], [0], [0], [1], [0], [1], [0], [0], [1], [0], [1], [1], [1], [1], [1], [0], [1], [1], [1], [0], [0], [0], [0], [1], [0], [1], [1], [0], [1], [1], [0], [0], [0], [0], [0], [0], [1], [0], [1], [1], [0], [1], [0], [0], [1], [1], [1], [0], [0], [1], [0], [0], [0], [0], [0], [0], [1], [0], [0], [0], [0], [0], [0], [0], [1], [0], [1], [1], [0], [0], [1], [0], [1], [1], [1], [1], [0], [0], [1], [0], [0], [0], [0], [0], [1], [0], [1], [1], [0], [1], [0], [1], [0], [1], [0], [0], [0], [0], [1], [1], [0], [0], [0], [1], [1], [0], [1], [0], [0], [1], [1], [1], [0], [1], [1], [0], [1], [0], [1], [1], [1], [0], [1], [1], [0], [1], [0], [1], [0], [1], [1], [0], [1], [0], [0], [0], [1], [0], [1], [0], [0], [0], [1], [1], [1], [0], [1], [0], [0], [1], [1], [1], [0], [0], [1], [0], [1], [0], [0], [0], [0], [0], [0], [0], [1], [0], [1], [1], [0], [0], [1], [1], [0], [0], [1], [0], [0], [0], [1], [0], [0], [1], [1], [0], [0], [1], [1], [0], [1], [0], [1], [1], [0], [1], [0], [0], [1], [1], [0], [0], [0], [1], [1], [1], [1], [1], [0], [1], [1], [0], [1], [0], [1], [1], [0], [1], [0], [0], [0], [1], [0], [1], [1], [0], [1], [0], [0], [1], [0], [1], [0], [0], [1], [1], [1], [1], [0], [1], [1], [1], [0], [0], [1], [1], [0], [1], [0], [0], [0], [1], [1], [0], [0], [1], [0], [0], [0], [1], [0], [1], [1], [1], [1], [1], [0], [0], [0], [1], [1], [1], [0], [1], [0], [1], [1], [0], [0], [1], [0], [0], [1], [1], [0], [0], [1], [0], [1], [0], [0], [1], [1], [1], [1], [0], [1], [1], [0], [0], [0], [1], [1], [1], [1], [0], [1], [1], [1], [0], [0], [1], [0], [1], [0], [0], [0], [1], [1], [1], [0], [1], [1], [0], [0], [1], [1], [1], [0], [1], [1], [1], [1], [0], [0], [0], [0], [0], [0], [1], [0], [0], [0], [0], [1], [1], [1], [1], [0], [0], [0], [1], [1], [0], [0], [0], [1], [1], [1], [1], [1], [0], [1], [1], [0], [0], [0], [0], [1], [0], [1], [1], [0], [0], [0], [0], [0], [0], [0], [0], [0], [1], [1], [0], [0], [0], [1], [1], [1], [0], [0], [1], [0], [1], [1], [1], [1], [0], [0], [0], [1], [0], [1], [1], [1], [0], [1], [0], [0], [0], [0], [0], [0], [1], [1], [0], [1], [1], [1], [1], [0], [0], [0], [0], [1], [1], [0], [1], [1], [1], [0], [0], [0], [1], [0], [0], [1], [0], [1], [1], [1], [0], [0], [1], [1], [0], [0], [0], [1], [0], [0], [1], [1], [0], [0], [1], [1], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [1], [1], [0], [1], [0], [0], [1], [0], [0], [1], [1], [0], [0], [0], [0], [1], [0], [0], [0], [1], [0], [0], [1], [0], [1], [1], [0], [0], [1], [1], [1], [0], [1], [0], [1], [0], [1], [0], [1], [1], [1], [0], [1], [0], [0], [1], [1], [1], [0], [1], [1], [1], [0], [0], [1], [0], [1], [1], [1], [1], [1], [1], [0], [1], [0], [0], [0], [0], [0], [1], [0], [0], [1], [1], [0], [1], [1], [1], [1], [1], [0], [0], [1], [1], [1], [0], [0], [0], [0], [1], [1], [1], [0], [0], [0], [1], [0], [0], [1], [0], [0], [1], [1], [0], [0], [0], [1], [1], [1], [1], [0], [0], [1], [1], [1], [1], [0], [0], [1], [1], [0], [1], [0], [0], [0], [1], [1], [1], [1], [1], [1], [1], [0], [1], [1], [0], [0], [0], [1], [0], [0], [0], [0], [1], [0], [1], [1], [0], [0], [1], [1], [1], [0], [1], [1], [1], [1], [1], [1], [1], [1], [0], [1], [1], [0], [0], [1], [1], [1], [0], [1], [1], [1], [1], [0], [1], [0], [0], [1], [0], [0], [1], [0], [0], [0], [0], [1], [0], [1], [1], [1], [0], [1], [0], [1], [0], [1], [0], [1], [1], [1], [0], [1], [0], [1], [1], [0], [0], [0], [1], [0], [0], [0], [0], [0], [1], [0], [0], [1], [0], [1], [1], [1], [0], [1], [0], [0], [0], [1], [1], [0], [1], [0], [0], [0], [1], [0], [1], [1], [0], [0], [0], [0], [0], [1], [1], [0], [1], [1], [0], [1], [0], [0], [1], [1], [1], [0], [0], [1], [0], [1], [0], [0], [1], [0], [1], [1], [0], [0], [0], [0], [0], [0], [1], [0], [0], [1], [0], [1], [0], [0], [1], [1], [0], [0], [0], [1], [1], [0], [1], [0], [1], [0], [1], [0], [1], [1], [0], [1], [0], [0], [0], [0], [0], [1], [0], [1], [0], [1], [1], [1], [1], [0], [0], [0], [0], [1], [1], [1], [0], [0], [1], [0], [1], [0], [1], [1], [1], [0], [0], [0], [1], [0], [0], [1], [0], [1], [1], [0], [0], [0], [0], [1], [0], [1], [1], [0], [1], [1], [0], [0], [1], [1], [0], [1], [0], [0], [1], [1], [1], [1], [1], [1], [0], [0], [1], [1], [1], [1], [0], [1], [1], [0], [0], [1], [1], [1], [0], [1], [1], [0], [0], [1], [0], [1], [0], [1], [0], [0], [1], [0], [1], [0], [1], [0], [1], [1], [1], [1], [1], [1], [0], [0], [1], [0], [0], [0], [0], [1], [0], [0], [1], [0], [0], [1], [0], [0], [0], [1], [0], [1], [1], [1], [1], [0], [1], [0], [0], [0], [0], [0], [0], [1], [1], [1], [0], [1], [0], [1], [0], [1], [0], [1], [1], [1], [1], [0], [0], [1], [0], [1], [1], [1], [0], [0], [0], [1], [0], [0], [0], [0], [1], [0], [1], [0], [0], [0], [0], [0], [1], [1], [1], [1], [1], [0], [0], [1], [1], [1], [0], [1], [1], [1], [0], [0], [0], [1], [0], [0], [0], [0], [0], [1], [0], [1], [1], [1], [1], [0], [1], [0], [0], [1], [0], [1], [1], [1], [1], [0], [1], [0], [0], [1], [1], [0], [0], [1], [0], [1], [0], [1], [1], [0], [1], [1], [0], [1], [0], [1], [1], [0], [1], [1], [0], [0], [0], [1], [0], [1], [0], [1], [1], [0], [0], [1], [0], [1], [1], [1], [1], [1], [0], [0], [0], [0], [0], [0], [0], [0], [1], [0], [0], [1], [0], [1], [1], [1], [1], [1], [0], [1], [0], [0], [0], [1], [1], [1], [0], [1], [0], [1], [1], [1], [1], [1], [1], [0], [0], [0], [1], [0], [1], [1], [0], [1], [1], [1], [0], [0], [1], [0], [0], [1], [0], [0], [0], [0], [1], [0], [0], [1], [1], [0], [1], [0], [1], [1], [1], [1], [1], [1], [0], [1], [0], [1], [1], [1], [0], [1], [0], [0], [0], [0], [1], [1], [0], [0], [0], [0], [0], [1], [0], [1], [1], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [1], [0], [1], [1], [0], [1], [0], [1], [1], [0], [0], [0], [1], [0], [0], [0], [1], [0], [1], [1], [0], [0], [0], [0], [1], [1], [1], [0], [1], [1], [1], [0], [0], [1], [1], [1], [0], [0], [1], [0], [1], [0], [1], [1], [0], [0], [0], [1], [1], [0], [1], [1], [0], [0], [1], [0], [1], [1], [1], [0], [0], [0], [1], [0], [1], [0], [1], [0], [1], [0], [0], [0], [0], [1], [1], [0], [0], [1], [0], [1], [0], [1], [0], [0], [0], [1], [0], [0], [1], [0], [1], [1], [1], [0], [0], [0], [0], [1], [0], [0], [0], [1], [0], [0], [0], [0], [0], [0], [0], [1], [0], [1], [1], [1], [0], [1], [0], [1], [0], [1], [1], [0], [0], [0], [1], [0], [1], [0], [1], [1], [0], [1], [0], [0], [1], [1], [1], [0], [1], [1], [0], [0], [0], [0], [1], [1], [0], [1], [1], [0], [0], [0], [0], [1], [1], [0], [1], [1], [0], [1], [1], [0], [1], [0], [0], [1], [0], [0], [0], [0], [1], [0], [1], [0], [1], [0], [0], [0], [1], [0], [1], [0], [0], [0], [1], [1], [0], [1], [1], [1], [1], [1], [0], [1], [0], [1], [1], [1], [1], [1], [1], [0], [0], [0], [1], [0], [0], [0], [1], [0], [0], [1], [0], [0], [0], [1], [0], [0], [0], [1], [1], [1], [1], [1], [0], [0], [0], [1], [0], [1], [1], [0], [0], [1], [0], [1], [0], [1], [0], [1], [1], [1], [1], [1], [1], [1], [0], [1], [1], [0], [0], [0], [0], [1], [1], [0], [0], [0], [0], [1], [1], [0], [0], [1], [0], [1], [0], [1], [0], [0], [1], [1], [0], [0], [1], [0], [1], [0], [1], [0], [1], [0], [0], [0], [1], [1], [1], [0], [1], [0], [0], [1], [0], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [0], [1], [1], [0], [0], [1], [1], [0], [1], [0], [1], [0], [1], [1], [1], [1], [1], [0], [0], [1], [0], [0], [1], [0], [1], [1], [1], [1], [0], [0], [0], [0], [1], [1], [1], [1], [1], [1], [0], [0], [1], [1], [0], [0], [1], [0], [1], [0], [0], [0], [1], [0], [1], [0], [0], [1], [0], [1], [0], [0], [1], [0], [1], [0], [1], [0], [0], [0], [0], [1], [1], [1], [0], [1], [1], [1], [1], [1], [0], [1], [1], [1], [0], [0], [0], [1], [1], [0], [0], [0], [0], [0], [1], [1], [0], [0], [1], [0], [0], [0], [0], [0], [0], [1], [0], [0], [0], [0], [0], [1], [0], [1], [1], [0], [1], [1], [0], [0], [0], [0], [0], [1], [1], [0], [0], [0], [1], [1], [0], [0], [0], [1], [0], [0], [0], [1], [1], [1], [1], [0], [0], [1], [0], [1], [1], [0], [1], [0], [1], [0], [1], [1], [0], [1], [1], [1], [0], [1], [1], [0], [0], [0], [0], [1], [1], [1], [1], [1], [1], [1], [0], [0], [1], [0], [1], [0], [1], [1], [1], [0], [0], [1], [0], [1], [1], [0], [1], [0], [1], [0], [1], [1], [1], [1], [0], [1], [1], [1], [0], [1], [1], [1], [0], [0], [1], [0], [0], [1], [0], [1], [1], [1], [0], [0], [1], [0], [1], [0], [1], [1], [1], [1], [1], [1], [0], [0], [0], [1], [0], [0], [1], [1], [1], [1], [0], [0], [1], [1], [1], [1], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [1], [1], [0], [1], [0], [0], [1], [0], [0], [1], [0], [0], [0], [0], [0], [0], [0], [0], [1], [1], [0], [0], [0], [0], [1], [1], [1], [0], [0], [0], [1], [0], [1], [1], [0], [0], [0], [0], [1], [1], [0], [1], [0], [0], [0], [0], [1], [0], [1], [1], [1], [1], [1], [1], [1], [1], [1], [0], [0], [0], [0], [1], [1], [1], [1], [0], [0], [0], [0], [0], [1], [1], [1], [0], [1], [0], [0], [1], [1], [1], [1], [1], [1], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [1], [0], [1], [1], [1], [1], [1], [0], [1], [0], [1], [1], [0], [1], [0], [1], [0], [1], [1], [0], [1], [0], [0], [0], [1], [1], [0], [1], [0], [1], [0], [0], [0], [1], [0], [0], [0], [1], [1], [0], [0], [0], [0], [0], [0], [1], [1], [1], [1], [0], [1], [0], [1], [0], [0], [0], [0], [0], [0], [1], [0], [0], [1], [1], [1], [1], [1], [1], [0], [1], [1], [1], [0], [1], [1], [1], [1], [0], [1], [1], [0], [1], [0], [1], [1], [1], [1], [0], [1], [0], [1], [0], [0], [1], [1], [1], [0], [0], [1], [1], [0], [0], [1], [0], [1], [1], [0], [0], [1], [0], [0], [0], [0], [0], [0], [1], [0], [1], [1], [0], [1], [1], [1], [0], [0], [1], [0], [1], [0], [1], [1], [0], [1], [1], [0], [0], [0], [1], [1], [1], [0], [0], [0], [0], [0], [1], [0], [1], [0], [0], [0], [1], [0], [0], [0], [0], [1], [0], [1], [1], [0], [0], [1], [0], [1], [0], [1], [0], [0], [0], [1], [0], [0], [1], [0], [0], [1], [0], [0], [1], [0], [1], [1], [0], [1], [0], [0], [1], [0], [0], [1], [1], [1], [0], [1], [1], [1], [1], [0], [1], [0], [0], [1], [1], [1], [0], [0], [0], [1], [1], [1], [0], [0], [1], [0], [0], [0], [0], [0], [0], [1], [1], [1], [1], [0], [1], [1], [1], [0], [1], [0], [1], [1], [1], [0], [0], [1], [1], [1], [0], [0], [0], [1], [0], [0], [0], [1], [0], [1], [1], [1], [1], [0], [0], [1], [0], [0], [0], [1], [0], [1], [1], [0], [1], [1], [0], [0], [1], [1], [1], [1], [0], [0], [0], [1], [1], [1], [0], [0], [1], [1], [1], [1], [0], [1], [0], [0], [0], [1], [1], [0], [0], [0], [1], [1], [1], [0], [0], [1], [1], [1], [0], [1], [0], [1], [0], [1], [1], [0], [0], [0], [0], [0], [0], [0], [0], [1], [0], [1], [0], [1], [1], [0], [0], [1], [1], [1], [1], [1], [0], [0], [1], [1], [1], [1], [0], [0], [1], [0], [0], [1], [1], [0], [0], [1], [0], [1], [1], [1], [1], [0], [1], [1], [1], [0], [0], [1], [1], [1], [1], [1], [0], [0], [1], [1], [1], [0], [1], [1], [0], [0], [0], [1], [1], [1], [1], [0], [1], [0], [1], [0], [0], [0], [1], [1], [1], [0], [0], [0], [1], [1], [0], [0], [0], [1], [1], [1], [0], [0], [1], [1], [0], [1], [1], [1], [1], [0], [1], [0], [0], [1], [1], [1], [1], [1], [0], [0], [1], [0], [1], [0], [1], [1], [0], [1], [0], [1], [1], [0], [1], [1], [0], [1], [0], [1], [1], [1], [0], [1], [1], [0], [0], [0], [0], [1], [0], [0], [1], [0], [0], [0], [1], [0], [0], [1], [1], [0], [0], [1], [1], [1], [0], [0], [1], [1], [1], [1], [1], [0], [0], [1], [1], [1], [1], [0], [0], [0], [0], [1], [1], [1], [1], [0], [0], [0], [0], [0], [0], [1], [1], [1], [1], [1], [0], [0], [1], [1], [1], [1], [1], [1], [0], [1], [0], [0], [1], [0], [1], [1], [1], [1], [0], [1], [1], [1], [1], [0], [0], [0], [0], [1], [1], [0], [1], [1], [0], [0], [0], [0], [1], [1], [1], [1], [0], [1], [1], [0], [0], [1], [1], [1], [0], [0], [1], [1], [1], [0], [0], [1], [1], [1], [1], [0], [0], [0], [0], [1], [1], [0], [1], [1], [1], [1], [0], [0], [0], [0], [1], [1], [0], [0], [1], [0], [1], [1], [0], [0], [0], [0], [0], [1], [0], [0], [0], [1], [0], [0], [1], [0], [1], [0], [0], [0], [0], [1], [1], [1], [0], [0], [1], [1], [1], [0], [0], [1], [1], [0], [1], [1], [0], [1], [0], [0], [1], [1], [0], [0], [1], [0], [1], [1], [1], [0], [1], [1], [0], [1], [1], [0], [0], [0], [1], [1], [0], [1], [1], [0], [1], [0], [0], [1], [1], [1], [1], [1], [0], [0], [0], [1], [0], [1], [1], [1], [1], [1], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [1], [1], [1], [1], [0], [1], [0], [0], [1], [0], [0], [1], [1], [1], [1], [0], [0], [1], [0], [0], [1], [0], [0], [1], [1], [0], [1], [0], [0], [0], [1], [1], [0], [0], [0], [1], [0], [1], [1], [1], [0], [0], [1], [1], [0], [1], [0], [0], [0], [0], [0], [1], [0], [1], [0], [1], [1], [0], [0], [1], [0], [1], [1], [1], [0], [0], [0], [0], [1], [1], [0], [1], [1], [1], [0], [1], [0], [0], [1], [1], [1], [0], [1], [0], [1], [0], [0], [0], [1], [1], [0], [0], [0], [0], [0], [0], [1], [1], [0], [0], [0], [0], [0], [1], [1], [0], [1], [1], [1], [1], [0], [0], [0], [1], [1], [0], [1], [0], [1], [0], [0], [0], [1], [0], [1], [1], [1], [1], [1], [0], [1], [1], [1], [1], [1], [1], [0], [1], [1], [1], [1], [0], [0], [0], [0], [1], [0], [0], [0], [1], [1], [1], [0], [1], [1], [1], [0], [0], [0], [1], [0], [1], [1], [0], [0], [0], [0], [0], [0], [1], [0], [1], [1], [0], [0], [0], [0], [0], [0], [1], [1], [0], [0], [1], [0], [0], [1], [0], [1], [1], [1], [1], [0], [0], [0], [1], [0], [1], [1], [1], [1], [0], [0], [0], [1], [1], [0], [0], [1], [0], [0], [0], [0], [1], [1], [0], [0], [1], [1], [0], [0], [1], [1], [1], [0], [0], [0], [1], [0], [0], [0], [1], [1], [1], [0], [1], [0], [0], [0], [1], [0], [0], [1], [1], [0], [0], [1], [1], [1], [1], [1], [0], [0], [0], [1], [1], [0], [0], [0], [0], [1], [1], [1], [1], [0], [0], [1], [1], [0], [1], [1], [0], [0], [1], [0], [0], [1], [0], [1], [1], [0], [1], [0], [1], [1], [0], [1], [1], [1], [0], [0], [0], [1], [0], [0], [1], [1], [0], [1], [0], [1], [1], [0], [0], [0], [1], [0], [0], [1], [0], [1], [1], [0], [0], [0], [0], [1], [0], [1], [1], [0], [1], [0], [1], [1], [0], [0], [1], [0], [0], [1], [1], [0], [0], [0], [1], [1], [1], [1], [1], [1], [0], [0], [0], [1], [0], [0], [0], [0], [0], [1], [1], [0], [0], [0], [1], [1], [0], [1], [0], [0], [1], [0], [1], [1], [1], [0], [1], [0], [0], [0], [0], [0], [1], [0], [0], [0], [0], [0], [1], [1], [1], [0], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [1], [1], [0], [0], [0], [1], [1], [0], [1], [0], [1], [1], [0], [0], [0], [0], [0], [0], [1], [0], [0], [1], [0], [1], [1], [1], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [1], [0], [1], [0], [1], [0], [1], [0], [0], [1], [0], [1], [0], [1], [1], [0], [1], [1], [0], [0], [0], [0], [1], [0], [0], [0], [1], [0], [1], [0], [1], [0], [0], [1], [1], [0], [0], [0], [0], [0], [0], [1], [1], [0], [1], [0], [1], [1], [0], [0], [1], [0], [0], [0], [1], [0], [0], [1], [1], [0], [1], [0], [0], [1], [0], [1], [0], [0], [1], [0], [1], [0], [1], [1], [0], [1], [1], [1], [1], [1], [1], [1], [0], [0], [0], [0], [1], [1], [1], [1], [0], [1], [1], [1], [0], [0], [0], [0], [1], [1], [0], [0], [1], [0], [0], [0], [1], [0], [0], [1], [1], [1], [0], [1], [0], [1], [0], [0], [0], [1], [1], [0], [0], [0], [0], [1], [1], [1], [0], [1], [0], [1], [0], [0], [1], [0], [1], [0], [0], [1], [0], [0], [1], [1], [0], [0], [0], [0], [0], [0], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [1], [1], [1], [1], [0], [1], [1], [1], [0], [1], [1], [1], [0], [0], [0], [0], [1], [0], [0], [0], [1], [0], [1], [0], [0], [1], [0], [1], [1], [1], [0], [1], [1], [0], [1], [1], [1], [0], [1], [1], [0], [0], [1], [0], [1], [1], [0], [1], [0], [0], [0], [1], [1], [0], [1], [0], [0], [1], [0], [0], [1], [0], [0], [0], [1], [1], [0], [1], [1], [0], [1], [0], [0], [1], [0], [0], [0], [0], [0], [1], [0], [1], [0], [1], [1], [0], [1], [0], [0], [1], [0], [1], [1], [0], [0], [1], [0], [1], [1], [0], [1], [1], [1], [1], [1], [0], [1], [1], [1], [0], [1], [1], [0], [1], [0], [1], [0], [0], [1], [1], [1], [1], [1], [1], [0], [0], [0], [0], [0], [0], [0], [1], [0], [1], [0], [0], [0], [1], [0], [1], [0], [1], [1], [1], [0], [1], [0], [0], [0], [1], [0], [1], [0], [0], [0], [0], [1], [0], [0], [1], [0], [1], [1], [0], [0], [1], [0], [1], [1], [0], [0], [1], [0], [0], [1], [1], [0], [0], [1], [1], [0], [0], [1], [1], [0], [1], [0], [0], [0], [0], [0], [0], [1], [0], [1], [1], [0], [1], [0], [0], [1], [0], [0], [1], [0], [1], [1], [1], [0], [1], [0], [1], [0], [1], [0], [1], [1], [0], [1], [1], [1], [1], [1], [0], [0], [0], [0], [1], [0], [1], [0], [0], [1], [0], [1], [0], [0], [0], [0], [0], [1], [1], [1], [0], [1], [0], [1], [1], [1], [1], [1], [1], [0], [0], [0], [0], [1], [0], [0], [0], [1], [0], [1], [1], [1], [0], [0], [0], [0], [0], [0], [1], [0], [1], [1], [0], [0], [1], [0], [0], [0], [0], [1], [0], [1], [1], [0], [0], [0], [0], [1], [0], [0], [1], [0], [0], [0], [0], [1], [0], [1], [0], [1], [1], [0], [0], [0], [0], [1], [1], [1], [1], [0], [0], [0], [0], [0], [0], [0], [1], [1], [0], [1], [1], [1], [1], [0], [0], [1], [0], [1], [1], [0], [0], [1], [1], [1], [1], [1], [0], [0], [0], [0], [0], [0], [0], [0], [0], [1], [0], [1], [0], [1], [1], [0], [0], [0], [1], [0], [0], [1], [0], [0], [1], [0], [0], [1], [0], [1], [1], [0], [1], [0], [1], [1], [0], [1], [0], [0], [1], [0], [1], [0], [0], [0], [1], [0], [1], [0], [0], [0], [0], [1], [0], [0], [1], [1], [0], [1], [0], [0], [0], [1], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [1], [1], [1], [1], [0], [1], [0], [1], [1], [1], [0], [1], [0], [0], [1], [0], [0], [0], [0], [1], [0], [0], [1], [0], [0], [0], [1], [1], [1], [1], [0], [0], [0], [0], [0], [1], [1], [1], [1], [0], [1], [1], [1], [0], [1], [1], [0], [1], [1], [1], [1], [1], [1], [0], [0], [0], [1], [0], [1], [0], [0], [0], [0], [0], [0], [1], [0], [0], [0], [0], [1], [1], [1], [1], [0], [0], [0], [1], [0], [1], [0], [1], [1], [0], [0], [1], [1], [0], [1], [1], [0], [1], [1], [0], [0], [1], [0], [1], [0], [0], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [0], [0], [1], [1], [1], [1], [1], [1], [0], [0], [0], [1], [0], [1], [1], [0], [1], [1], [1], [1], [1], [0], [1], [0], [1], [0], [1], [1], [0], [1], [0], [0], [0], [1], [0], [1], [1], [0], [0], [0], [0], [1], [1], [0], [1], [1], [1], [1], [0], [0], [0], [1], [1], [1], [1], [1], [0], [1], [0], [0], [0], [0], [0], [0], [0], [1], [0], [0], [0], [1], [1], [1], [0], [1], [1], [0], [0], [0], [0], [1], [0], [0], [1], [0], [1], [1], [1], [1], [0], [1], [0], [0], [0], [1], [0], [1], [1], [1], [0], [0], [0], [1], [0], [1], [1], [1], [1], [0], [1], [1], [0], [1], [1], [0], [0], [0], [1], [1], [0], [0], [1], [0], [1], [0], [1], [1], [0], [0], [0], [0], [1], [1], [1], [0], [0], [1], [0], [0], [0], [1], [1], [1], [0], [1], [1], [0], [0], [0], [0], [0], [1], [0], [1], [1], [0], [0], [1], [1], [0], [0], [1], [0], [0], [0], [1], [0], [1], [0], [1], [1], [1], [1], [0], [1], [1], [1], [1], [1], [1], [0], [1], [0], [0], [1], [1], [0], [1], [0], [1], [0], [1], [0], [0], [1], [1], [0], [0], [0], [1], [0], [1], [0], [1], [0], [0], [1], [0], [0], [0], [1], [0], [1], [1], [0], [1], [0], [0], [1], [0], [1], [1], [0], [1], [1], [1], [1], [1], [1], [0], [0], [1], [1], [0], [1], [1], [1], [0], [1], [1], [0], [1], [0], [1], [1], [1], [0], [1], [1], [1], [0], [0], [1], [0], [0], [1], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [1], [0], [0], [0], [1], [0], [1], [1], [1], [1], [1], [1], [0], [0], [1], [1], [0], [0], [0], [0], [1], [1], [1], [1], [1], [0], [1], [0], [0], [0], [0], [1], [0], [0], [1], [0], [1], [1], [0], [0], [0], [0], [1], [0], [0], [0], [1], [1], [1], [0], [0], [1], [1], [1], [0], [0], [1], [0], [0], [1], [0], [1], [0], [0], [0], [1], [0], [1], [1], [0], [0], [1], [1], [0], [1], [1], [0], [1], [1], [1], [0], [1], [1], [0], [0], [0], [1], [1], [0], [0], [1], [0], [1], [1], [1], [0], [1], [1], [0], [0], [0], [0], [1], [1], [1], [1], [0], [1], [0], [1], [1], [0], [0], [1], [0], [1], [1], [0], [0], [1], [0], [1], [0], [0], [0], [1], [1], [0], [1], [1], [0], [0], [0], [0], [0], [1], [0], [0], [1], [0], [1], [0], [1], [0], [0], [0], [1], [1], [1], [1], [0], [0], [1], [1], [1], [1], [0], [0], [1], [0], [0], [1], [1], [0], [1], [1], [1], [0], [0], [0], [1], [1], [0], [0], [0], [0], [0], [1], [1], [1], [1], [0], [1], [0], [1], [0], [1], [1], [0], [1], [0], [1], [0], [1], [0], [0], [0], [0], [1], [0], [1], [1], [1], [1], [1], [0], [0], [0], [0], [1], [0], [0], [0], [1], [0], [1], [1], [1], [1], [1], [1], [0], [0], [0], [0], [0], [1], [0], [1], [1], [0], [1], [1], [0], [1], [0], [1], [1], [1], [0], [1], [0], [0], [0], [0], [1], [1], [0], [0], [1], [1], [0], [1], [1], [1], [1], [1], [0], [0], [0], [0], [1], [0], [0], [0], [0], [0], [1], [0], [0], [0], [1], [1], [1], [1], [0], [0], [1], [1], [0], [1], [1], [0], [1], [1], [0], [0], [1], [1], [0], [1], [0], [1], [0], [0], [0], [1], [0], [1], [1], [1], [1], [0], [1], [0], [0], [1], [1], [0], [0], [0], [1], [1], [1], [0], [1], [0], [0], [0], [1], [0], [0], [1], [0], [0], [0], [1], [1], [0], [1], [0], [1], [1], [1], [0], [1], [1], [1], [1], [1], [1], [1], [1], [0], [1], [0], [1], [0], [0], [0], [1], [0], [0], [0], [0], [1], [1], [1], [0], [1], [1], [0], [1], [1], [1], [1], [0], [0], [0], [0], [1], [1], [1], [1], [0], [0], [0], [1], [0], [1], [1], [1], [1], [1], [0], [0], [1], [1], [1], [1], [0], [0], [0], [0], [0], [0], [0], [1], [1], [0], [0], [0], [0], [0], [1], [1], [1], [1], [1], [1], [1], [0], [0], [0], [0], [1], [0], [0], [1], [1], [1], [1], [0], [0], [0], [1], [0], [1], [1], [0], [0], [0], [1], [0], [1], [1], [1], [0], [0], [1], [0], [1], [1], [1], [1], [1], [1], [0], [1], [1], [0], [0], [1], [1], [1], [0], [1], [0], [0], [0], [0], [0], [1], [0], [1], [1], [1], [1], [0], [0], [1], [0], [1], [0], [1], [0], [1], [1], [1], [0], [0], [0], [1], [1], [1], [1], [0], [1], [0], [0], [1], [0], [1], [0], [0], [0], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [0], [0], [0], [0], [1], [0], [0], [0], [1], [1], [1], [1], [1], [0], [1], [1], [0], [1], [0], [1], [0], [1], [1], [0], [0], [1], [1], [1], [0], [1], [0], [1], [1], [1], [0], [0], [1], [1], [0], [1], [0], [0], [1], [0], [0], [0], [0], [0], [0], [1], [0], [1], [0], [1], [1], [0], [1], [0], [0], [0], [1], [1], [1], [0], [0], [1], [1], [1], [1], [1], [0], [1], [1], [0], [0], [1], [1], [1], [1], [1], [0], [1], [1], [0], [1], [0], [1], [0], [0], [0], [0], [1], [1], [0], [1], [1], [0], [0], [1], [0], [1], [1], [1], [0], [0], [1], [0], [1], [0], [1], [1], [1], [0], [1], [0], [1], [1], [0], [0], [1], [1], [0], [1], [1], [0], [1], [1], [1], [1], [1], [1], [1], [1], [0], [0], [0], [0], [0], [0], [0], [1], [1], [1], [1], [1], [1], [1], [0], [0], [1], [1], [1], [0], [1], [1], [1], [0], [1], [1], [0], [0], [1], [0], [0], [1], [1], [0], [0], [0], [1], [1], [0], [1], [1], [1], [1], [1], [0], [1], [0], [1], [0], [0], [1], [0], [0], [1], [0], [1], [0], [1], [0], [1], [1], [1], [1], [0], [0], [0], [1], [0], [0], [0], [0], [0], [0], [0], [1], [1], [0], [0], [0], [1], [0], [1], [0], [0], [0], [1], [0], [0], [1], [0], [0], [1], [0], [1], [0], [0], [0], [0], [1], [1], [0], [0], [1], [1], [1], [1], [1], [1], [1], [1], [0], [1], [1], [0], [1], [0], [0], [0], [1], [1], [1], [1], [1], [1], [1], [1], [0], [1], [1], [1], [1], [1], [0], [1], [0], [0], [1], [1], [1], [1], [0], [0], [1], [1], [1], [1], [0], [0], [0], [1], [1], [1], [1], [1], [1], [0], [0], [1], [0], [1], [1], [1], [0], [0], [1], [1], [1], [0], [0], [0], [0], [0], [0], [1], [0], [0], [0], [1], [0], [1], [1], [0], [0], [0], [0], [0], [0], [1], [0], [1], [1], [1], [0], [1], [1], [0], [1], [1], [1], [0], [0], [0], [1], [1], [1], [0], [0], [0], [1], [1], [0], [0], [1], [1], [0], [0], [0], [0], [0], [1], [1], [0], [1], [1], [1], [0], [0], [0], [1], [0], [0], [1], [0], [0], [0], [1], [0], [1], [1], [0], [0], [0], [1], [1], [0], [0], [1], [0], [1], [0], [0], [0], [1], [1], [0], [0], [1], [1], [0], [0], [1], [0], [0], [1], [0], [0], [1], [0], [0], [0], [1], [1], [0], [1], [0], [1], [0], [0], [1], [0], [0], [1], [0], [0], [1], [0], [1], [0], [0], [0], [1], [0], [0], [1], [1], [0], [0], [1], [1], [0], [1], [1], [1], [1], [1], [1], [0], [0], [0], [1], [0], [1], [0], [0], [0], [1], [0], [1], [0], [1], [1], [0], [0], [0], [0], [1], [1], [0], [1], [0], [0], [0], [1], [1], [1], [1], [0], [1], [1], [1], [1], [0], [0], [0], [0], [0], [0], [1], [1], [1], [1], [1], [1], [0], [0], [1], [0], [0], [1], [1], [0], [0], [0], [1], [0], [0], [0], [1], [1], [1], [1], [1], [0], [0], [0], [1], [0], [1], [0], [1], [1], [0], [1], [0], [1], [0], [1], [1], [0], [0], [0], [1], [0], [1], [0], [1], [0], [0], [1], [0], [1], [1], [0], [1], [1], [0], [0], [0], [0], [1], [0], [1], [1], [0], [1], [1], [0], [1], [1], [1], [0], [1], [1], [0], [0], [1], [0], [0], [1], [0], [0], [1], [0], [1], [0], [0], [0], [0], [0], [0], [1], [0], [0], [0], [0], [0], [1], [0], [1], [1], [1], [0], [0], [0], [1], [1], [1], [0], [0], [1], [0], [1], [1], [0], [0], [0], [1], [1], [0], [1], [1], [1], [0], [0], [0], [0], [1], [0], [1], [1], [1], [0], [0], [0], [1], [0], [0], [0], [1], [1], [1], [0], [1], [0], [0], [1], [0], [1], [1], [1], [1], [1], [1], [0], [0], [0], [0], [0], [1], [1], [0], [1], [1], [0], [0], [0], [1], [1], [1], [1], [1], [0], [1], [1], [1], [1], [1], [1], [1], [0], [1], [0], [0], [0], [1], [1], [1], [1], [0], [1], [0], [1], [1], [0], [0], [1], [1], [0], [0], [1], [0], [1], [1], [1], [1], [0], [0], [1], [1], [1], [0], [0], [0], [1], [0], [1], [0], [0], [1], [0], [1], [0], [0], [0], [0], [1], [1], [0], [0], [1], [1], [1], [1], [0], [1], [0], [1], [0], [1], [1], [1], [0], [0], [1], [0], [1], [0], [0], [0], [0], [0], [1], [1], [0], [0], [1], [1], [0], [0], [1], [0], [1], [0], [1], [0], [1], [0], [0], [1], [0], [1], [1], [0], [0], [1], [0], [0], [1], [1], [0], [1], [0], [0], [1], [0], [0], [1], [0], [1], [1], [1], [0], [0], [1], [0], [0], [1], [1], [0], [1], [1], [0], [1], [1], [0], [0], [0], [0], [0], [0], [1], [1], [1], [0], [0], [0], [1], [1], [0], [1], [1], [1], [0], [0], [0], [1], [0], [1], [1], [1], [1], [1], [0], [1], [1], [1], [0], [0], [1], [0], [0], [1], [1], [0], [1], [1], [1], [0], [1], [1], [0], [0], [1], [0], [1], [1], [0], [0], [0], [0], [0], [1], [0], [1], [0], [0], [1], [0], [0], [0], [1], [0], [0], [0], [1], [0], [1], [0], [0], [1], [1], [1], [1], [1], [0], [0], [0], [1], [0], [1], [1], [1], [1], [1], [1], [0], [0], [1], [1], [0], [1], [0], [0], [0], [0], [0], [1], [0], [0], [0], [1], [1], [0], [1], [0], [0], [0], [0], [1], [1], [1], [1], [1], [0], [1], [0]]\n"
     ]
    }
   ],
   "source": [
    "print(train_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E5NZIsmiGmoc"
   },
   "source": [
    "For implementation purposes, we will need an index for the padded word and we will use the index 19757.\n",
    "Note: For a dataset of this <i>small</i> size, we will need to do K-Fold Cross-validation to evaluate the performance. However, we will work with this train-test split for the rest of this assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cIWGatDXGmoc"
   },
   "source": [
    "## Simple Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BQEg38m6Gmod"
   },
   "source": [
    "<img src=\"https://web.cs.dal.ca/~sastry/cnn_simple.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZCvl-t5EGmoe"
   },
   "source": [
    "The above image shows the architecture of the simple model that we will implement for text classification. We are interested in the following hyperparameters apart from the number of filters (which we will set to 1 for this problem):\n",
    "* The span of the filter/the number of words considered for making the prediction.\n",
    "* The size of the stride.\n",
    "* The number of activations selected for feeding into softmax classifier.\n",
    "\n",
    "Before continuing:\n",
    "\n",
    "* Can you reason how the machine is classifying (in the above example)? The values of the activations are color-coded. Is this the only possible way the machine can work? \n",
    "\n",
    "    (Your answer might look like : ... filter is ... template matching ... )\n",
    "\n",
    "<b>Answer</b>: The convolution process in the above mentioned example is helping the network to learn the correlation between the adjascent inputs (which are words). When the filter convolves over the embedding matrix of a sentence, a feature tesnor is generated (The brown color grid after convolution). It does have some activations which have high values and some activations having low values (color-coded vector). We selected the maximum activation (from groups of two) beacuse they are having a greater impact on the output. For a number of training examples, the machine looks for words that thave a larger impact on the class (words with negative and positive tones) and their embeddings are updated accordingly. \n",
    "    \n",
    "* Why might order of activations need to be retained?\n",
    "\n",
    "<b>Answer</b>: The order of activations might be retained beacuse of the order of the words in the sentence. In a sentence the order of the words is important and most of the time, the literal meaning of a word is replaced by contextual meaning. Retaining the order of activation will assist in learning the contextual meaning of the underlying word in the sentence.\n",
    "\n",
    "* In the code, we will add an additional row of zeros to represent the padded words. Will the zeros of the padded words be updated during back-prop? Why or why not?\n",
    "\n",
    "<b>Answer</b>: Yes the zeros in pre trained embedding matrix were getting updated. But when we put trainable=False  in zeros varaible that had to be added in embedding matrix then it were not getting trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JrDDcU5GGmoe"
   },
   "source": [
    "First, we will write code which can select k top elements in the order they appeared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "8SViy9l_Gmog"
   },
   "outputs": [],
   "source": [
    "def k_max_pool(A,k):\n",
    "    \"\"\"\n",
    "    A = 2 dimensional array (assume that the length of last dimension of A will be always more than k)\n",
    "    k = number of elements.\n",
    "    Return: For every row of A, top k elements in the order they appear.\n",
    "    \"\"\"\n",
    "    assert len(A.get_shape())==2\n",
    "    def func(row):\n",
    "        \"\"\"\n",
    "        Hint : I used top_k and reverse.\n",
    "        I am not sure whether the order of the indices are retained when sorted = False in top_k. (did not find any documentation)\n",
    "        Therefore, I suggest that you sort the indices before selecting the elements from the array(Trick: use top_k again!)\"\"\"\n",
    "        ret_tensor = None\n",
    "        ## your code here to compute ret_tensor ##\n",
    "        index = tf.nn.top_k(row, k=k).indices\n",
    "        index = tf.contrib.framework.sort(index,axis=-1,direction='ASCENDING',name=None)\n",
    "        ret_tensor=tf.gather(row,index)\n",
    "        \n",
    "        return ret_tensor\n",
    "    return tf.map_fn(func,A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2714,
     "status": "ok",
     "timestamp": 1529106152321,
     "user": {
      "displayName": "Nikhil Dhirmalani",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "117732879546167846741"
     },
     "user_tz": 180
    },
    "id": "MayRnDIAGmoi",
    "outputId": "e4a5f81c-2570-41ed-e9b3-9207f3b25355"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "A = tf.placeholder(shape=[None,None],dtype=tf.float64)\n",
    "top = k_max_pool(A,5)\n",
    "sess = tf.Session()\n",
    "for i in range(1,6):\n",
    "    np.random.seed(5)\n",
    "    l = np.random.randn(i*10,i*10)\n",
    "    top_elements = sess.run(top,feed_dict={A:l})\n",
    "    l = l.tolist()\n",
    "    top_elements2 = np.array(map(lambda x: [x[i] for i in range(len(x)) if x[i]>sorted(x,reverse=True)[5]],l))\n",
    "    # Note that this test assumes that the 6th largest element and 5th largest element are different.\n",
    "    print(((top_elements - top_elements2)<10**-10).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KN66FnNcGmok"
   },
   "outputs": [],
   "source": [
    "def initializer(shape):\n",
    "    xavier = tf.contrib.layers.xavier_initializer(seed=1)\n",
    "    return xavier(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "HeXmr83YGmom"
   },
   "outputs": [],
   "source": [
    "class CNN_simple:\n",
    "    def __init__(self, num_words, embedding_size=30, span=2, k=5):\n",
    "        self.num_words = num_words\n",
    "\n",
    "        # The batch of text documents. Let's assume that it is always padded to length 100.\n",
    "        # We could use [None,None], but we'll use [None,100] for simplicity.\n",
    "        self.input = tf.placeholder(shape=[None, 100], dtype=tf.int32)\n",
    "        self.expected_output = tf.placeholder(shape=[None, 1], dtype=tf.int32)\n",
    "\n",
    "        embedding_matrix = tf.Variable(initializer((num_words, embedding_size)), name=\"embeddings\")\n",
    "        # Add an additional row of zeros to denote padded words.\n",
    "        v2 = tf.Variable(tf.zeros([1, embedding_size]), trainable=False,dtype=tf.float32)\n",
    "\n",
    "        self.embedding_matrix = tf.concat([embedding_matrix,v2],axis=0)\n",
    "        \n",
    "        # Extract the vectors from the embedding matrix. The dimensions should be None x 100 x embedding_size. \n",
    "        vectors = tf.nn.embedding_lookup(self.embedding_matrix,self.input) # None x 100 x embedding_size\n",
    "        \n",
    "        # In order to use conv2d, we need vectors to be 4 dimensional.\n",
    "        # The convention is NHWC - None (Batch Size) x Height(Height of image) x Width(Width of image) x Channel(Depth - similar to RGB).\n",
    "        # For text, let's consider Height = 1, width = number of words, channel = embedding_size\n",
    "        vectors2d = tf.expand_dims(vectors,1) # None x 1 x 100 x embedding_size\n",
    "        \n",
    "        # Conv2d needs a filter bank.\n",
    "        # The dimensions of the filter bank = Height, Width, in-channels, out-channels(Number-of-Filters).\n",
    "        # We are creating a single filter of size = span. \n",
    "        # So, height = 1, width = span, in-channels = embedding_size ,out-channels = 1. \n",
    "        single_filter = tf.Variable(initializer((1, span, embedding_size, 1)), name=\"filter\")  \n",
    "        bias = tf.Variable(0.0,name=\"bias\")\n",
    "        conv_span = tf.nn.conv2d(\n",
    "            input=vectors2d,\n",
    "            filter=single_filter,\n",
    "            # Note that the first and last elements SHOULD be 1. \n",
    "            strides=[1, 1, 1, 1], \n",
    "            # This means that we are ok with input size being reduced during the process of convolution.\n",
    "            padding=\"VALID\"\n",
    "        ) # None x 1 x (100-span+1) x 1\n",
    "        \n",
    "        acts = tf.nn.leaky_relu(conv_span+bias)\n",
    "        \n",
    "        # Now, let us extract the top k activations. \n",
    "        # But, we need to first convert acts this into 2-dimensional. \n",
    "        acts_2d = tf.squeeze(acts,squeeze_dims=[1,3])\n",
    "        \n",
    "        # Use k_max_pool to extract top-k activations\n",
    "        input_fully_connected = k_max_pool(acts_2d,k) # None x k\n",
    "    \n",
    "        self.softmax_weight = tf.Variable(initializer((k, 2)))\n",
    "        self.softmax_bias = tf.Variable(0.0 * initializer((1, 2)))\n",
    "        logits = tf.matmul(input_fully_connected, self.softmax_weight) + self.softmax_bias\n",
    "        # Write out the equation for computing the logits.\n",
    "        self.output = tf.nn.softmax(logits, axis=1)  # Shape = ?\n",
    "\n",
    "        # Compute the cross-entropy cost.\n",
    "        # You might either sum or take mean of all the costs across all the examples.\n",
    "        # It is your choice as the test case is on Stochastic Training.\n",
    "        entropy = tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(self.expected_output, 2), logits=logits)\n",
    "        self.cost = tf.reduce_mean(entropy)\n",
    "        \n",
    "#         self.output = tf.nn.softmax(tf.matmul(input_fully_connected, self.softmax_weight) + self.softmax_bias, axis=1)\n",
    "        \n",
    "#         self.cost = -tf.reduce_sum((self.expected_output * tf.log(self.output[:, 1]) +\n",
    "#                                (1 - self.expected_output) * tf.log(self.output[:, 0])))\n",
    "        \n",
    "        correct_prediction = tf.equal(tf.reshape(tf.argmax(self.output, 1),[-1,1]), tf.cast(self.expected_output, dtype=tf.int64))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))        \n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        self.train_op = optimizer.minimize(self.cost)\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "\n",
    "    def pad(self, data, pad_word, pad_length=100):\n",
    "        for datum in data:\n",
    "            datum.extend([pad_word] * (pad_length - len(datum)))\n",
    "        return data\n",
    "\n",
    "    def train(self, train_data, test_data, train_targets, test_targets, batch_size=1, epochs=1, verbose=False):\n",
    "        sess = self.session\n",
    "        self.pad(train_data, self.num_words)\n",
    "        self.pad(test_data, self.num_words)\n",
    "        print(\"Starting training...\")\n",
    "        for epoch in range(epochs):\n",
    "            cost_epoch = 0\n",
    "            c = 0\n",
    "            for datum, target in zip([train_data[i:i + batch_size] for i in range(0, len(train_data), batch_size)],\n",
    "                                     [train_targets[i:i + batch_size] for i in\n",
    "                                      range(0, len(train_targets), batch_size)]):\n",
    "                _, cost,output = sess.run([self.train_op, self.cost,self.output],\n",
    "                                   feed_dict={self.input: datum, self.expected_output: target})\n",
    "                cost_epoch += cost\n",
    "                c += 1\n",
    "                \n",
    "                if c % 100 == 0 and verbose:\n",
    "#                     print(sess.run(self.embedding_matrix[-1]))\n",
    "                    print(\"\\t{} batches finished. Cost : {}\".format(c, cost_epoch / c))\n",
    "            print(\"Epoch {}: {}\".format(epoch, cost_epoch / len(train_data)))\n",
    "            print(\"\\tTrain accuracy: {}\".format(self.compute_accuracy(train_data, train_targets)))\n",
    "            print(\"\\tTest accuracy: {}\".format(self.compute_accuracy(test_data, test_targets)))\n",
    "\n",
    "    def compute_accuracy(self, data, targets):\n",
    "        return self.session.run(self.accuracy, feed_dict={self.input: data, self.expected_output: targets})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1666
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 68401,
     "status": "ok",
     "timestamp": 1529105394952,
     "user": {
      "displayName": "Prashant Pandey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "110318069328577762879"
     },
     "user_tz": 180
    },
    "id": "KN8RoSLoGmoo",
    "outputId": "7f7c05af-ff7f-42c1-f399-4d1eae44ee83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-5fbf38e41785>:57: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Starting training...\n",
      "\t100 batches finished. Cost : 0.688363171816\n",
      "\t200 batches finished. Cost : 0.695461699367\n",
      "\t300 batches finished. Cost : 0.69590206484\n",
      "\t400 batches finished. Cost : 0.697339066267\n",
      "\t500 batches finished. Cost : 0.698220440388\n",
      "\t600 batches finished. Cost : 0.697462846041\n",
      "\t700 batches finished. Cost : 0.697759573374\n",
      "\t800 batches finished. Cost : 0.697326148674\n",
      "\t900 batches finished. Cost : 0.696654114458\n",
      "\t1000 batches finished. Cost : 0.696934164703\n",
      "\t1100 batches finished. Cost : 0.696292639321\n",
      "\t1200 batches finished. Cost : 0.6951062045\n",
      "\t1300 batches finished. Cost : 0.695746983336\n",
      "\t1400 batches finished. Cost : 0.696385819912\n",
      "\t1500 batches finished. Cost : 0.695639149825\n",
      "\t1600 batches finished. Cost : 0.695392676797\n",
      "\t1700 batches finished. Cost : 0.695754948697\n",
      "\t1800 batches finished. Cost : 0.695977645831\n",
      "\t1900 batches finished. Cost : 0.695701944185\n",
      "\t2000 batches finished. Cost : 0.695247680426\n",
      "\t2100 batches finished. Cost : 0.694428735517\n",
      "\t2200 batches finished. Cost : 0.694853377383\n",
      "\t2300 batches finished. Cost : 0.694558171384\n",
      "\t2400 batches finished. Cost : 0.694085530937\n",
      "\t2500 batches finished. Cost : 0.693020979023\n",
      "\t2600 batches finished. Cost : 0.692662136486\n",
      "\t2700 batches finished. Cost : 0.693177666543\n",
      "\t2800 batches finished. Cost : 0.692366171149\n",
      "\t2900 batches finished. Cost : 0.691678471791\n",
      "\t3000 batches finished. Cost : 0.691966537148\n",
      "\t3100 batches finished. Cost : 0.69225746075\n",
      "\t3200 batches finished. Cost : 0.691529853567\n",
      "\t3300 batches finished. Cost : 0.691115452638\n",
      "\t3400 batches finished. Cost : 0.691398807256\n",
      "\t3500 batches finished. Cost : 0.691416782737\n",
      "\t3600 batches finished. Cost : 0.691391655323\n",
      "\t3700 batches finished. Cost : 0.691068911552\n",
      "\t3800 batches finished. Cost : 0.690949390711\n",
      "\t3900 batches finished. Cost : 0.690517378472\n",
      "\t4000 batches finished. Cost : 0.690368129507\n",
      "\t4100 batches finished. Cost : 0.690143609592\n",
      "\t4200 batches finished. Cost : 0.690339886809\n",
      "\t4300 batches finished. Cost : 0.690389276743\n",
      "\t4400 batches finished. Cost : 0.690553110411\n",
      "\t4500 batches finished. Cost : 0.690163681726\n",
      "\t4600 batches finished. Cost : 0.68981139772\n",
      "\t4700 batches finished. Cost : 0.689431945916\n",
      "\t4800 batches finished. Cost : 0.689024873289\n",
      "\t4900 batches finished. Cost : 0.688819039452\n",
      "\t5000 batches finished. Cost : 0.688381806445\n",
      "\t5100 batches finished. Cost : 0.688175228507\n",
      "\t5200 batches finished. Cost : 0.688049943063\n",
      "\t5300 batches finished. Cost : 0.687438213179\n",
      "\t5400 batches finished. Cost : 0.68726203079\n",
      "\t5500 batches finished. Cost : 0.687195945767\n",
      "\t5600 batches finished. Cost : 0.686344007786\n",
      "\t5700 batches finished. Cost : 0.686179788003\n",
      "\t5800 batches finished. Cost : 0.685655898223\n",
      "\t5900 batches finished. Cost : 0.685972148056\n",
      "\t6000 batches finished. Cost : 0.685690100017\n",
      "\t6100 batches finished. Cost : 0.685392777219\n",
      "\t6200 batches finished. Cost : 0.684812547437\n",
      "\t6300 batches finished. Cost : 0.684353346337\n",
      "\t6400 batches finished. Cost : 0.684047539849\n",
      "\t6500 batches finished. Cost : 0.683380797877\n",
      "\t6600 batches finished. Cost : 0.683325827745\n",
      "\t6700 batches finished. Cost : 0.682583962361\n",
      "\t6800 batches finished. Cost : 0.681868452778\n",
      "\t6900 batches finished. Cost : 0.682024423033\n",
      "\t7000 batches finished. Cost : 0.681632114274\n",
      "\t7100 batches finished. Cost : 0.680960620493\n",
      "\t7200 batches finished. Cost : 0.680770793754\n",
      "\t7300 batches finished. Cost : 0.680385472123\n",
      "\t7400 batches finished. Cost : 0.679857551578\n",
      "\t7500 batches finished. Cost : 0.679272236707\n",
      "\t7600 batches finished. Cost : 0.678875324834\n",
      "\t7700 batches finished. Cost : 0.678377051984\n",
      "\t7800 batches finished. Cost : 0.678056771599\n",
      "\t7900 batches finished. Cost : 0.678070607002\n",
      "\t8000 batches finished. Cost : 0.677414153557\n",
      "\t8100 batches finished. Cost : 0.676894012192\n",
      "\t8200 batches finished. Cost : 0.676402013377\n",
      "\t8300 batches finished. Cost : 0.675500186195\n",
      "\t8400 batches finished. Cost : 0.674959766094\n",
      "\t8500 batches finished. Cost : 0.674827589333\n",
      "Epoch 0: 0.675099696684\n",
      "\tTrain accuracy: 0.718958854675\n",
      "\tTest accuracy: 0.664322555065\n"
     ]
    }
   ],
   "source": [
    "c=CNN_simple(len(vocab))\n",
    "c.train(train_data,test_data,train_targets,test_targets,epochs=1,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9qAcWbzyGmop"
   },
   "source": [
    "The expected output for the above snippet is\n",
    "<pre>\n",
    "Starting training...\n",
    "\t100 batches finished. Cost : 0.688363179564\n",
    "\t200 batches finished. Cost : 0.695461705327\n",
    "\t300 batches finished. Cost : 0.695902070602\n",
    "\t400 batches finished. Cost : 0.697339072227\n",
    "\t500 batches finished. Cost : 0.698220448136\n",
    "    ...\n",
    "Epoch 0: 0.675099702418\n",
    "\tTrain accuracy: 0.718958854675\n",
    "\tTest accuracy: 0.664322555065   \n",
    "</pre>\n",
    "If you get any other output and you feel you are correct, you can proceed (However, I cannot think of any case where you can get a different output). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rtV-0gInGmoq"
   },
   "source": [
    "## ConvNet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xi_un8GvGmor"
   },
   "source": [
    "\n",
    "### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FtrPWOepGmor"
   },
   "source": [
    "<img src=\"https://web.cs.dal.ca/~sastry/cnn.png\" style=\"height:40%;width:40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WLMzzdMBGmos"
   },
   "source": [
    "Essentially, there are 2 kind of hyper-parameters - the filter size and number of filters of each size. In the image shown, there are 3 filter-sizes - 2,3,4 and number of filters of each size is 2. Once the convolution is obtained, 1-max pooling is done - it basically involves extracting 1 activation from the list of activations which is the maximum activation. The reason we need to do this is to construct the inputs to the softmax layer which are of a fixed size.\n",
    "Read more at https://arxiv.org/pdf/1510.03820.pdf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "sqoRWB9wGmot"
   },
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self, num_words, embedding_size=30):\n",
    "        self.num_words = num_words\n",
    "\n",
    "        # The batch of text documents. Let's assume that it is always padded to length 100.\n",
    "        # We could use [None,None], but we'll use [None,100] for simplicity.\n",
    "        self.input = tf.placeholder(shape=[None, 100], dtype=tf.int32)\n",
    "        self.expected_output = tf.placeholder(shape=[None, 1], dtype=tf.int32)\n",
    "\n",
    "        embedding_matrix = tf.Variable(initializer((num_words, embedding_size)), name=\"embeddings\")\n",
    "        # Add an additional row of zeros to denote padded words.\n",
    "        v2 = tf.Variable(tf.zeros([1, embedding_size]), dtype=tf.float32)\n",
    "\n",
    "        self.embedding_matrix = tf.concat([embedding_matrix, 0*v2], 0)\n",
    "\n",
    "        # Extract the vectors from the embedding matrix. The dimensions should be None x 100 x embedding_size.\n",
    "        # Use embedding lookup\n",
    "        vectors = tf.nn.embedding_lookup(self.embedding_matrix, self.input)  # None x 100 x embedding_size\n",
    "\n",
    "        # In order to use conv2d, we need vectors to be 4 dimensional.\n",
    "        # The convention is NHWC - None (Batch Size) x Height(Height of image) x Width(Width of image) x Channel(Depth - similar to RGB).\n",
    "        # For text, let's consider Height = 1, width = number of words, channel = embedding_size.\n",
    "        # Use expand-dims to modify.\n",
    "        vectors2d = tf.expand_dims(vectors, 1)  # None x 1 x 100 x embedding_size\n",
    "\n",
    "        # Create 50 filters with span of 3 words. You need 1 bias for each filter.\n",
    "        filter_tri = tf.Variable(initializer((1, 3, embedding_size, 50)), name=\"weight3\")\n",
    "        bias_tri = tf.Variable(tf.zeros((1, 50)), name=\"bias3\")\n",
    "        conv1 = tf.nn.conv2d(\n",
    "            input=vectors2d,\n",
    "            filter=filter_tri,\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding=\"VALID\"\n",
    "        )  # Shape = (None x 1 x 98 x 50)\n",
    "        A1 = tf.nn.leaky_relu(conv1 + bias_tri)\n",
    "   \n",
    "\n",
    "        # Create 50 filters with span of 4 words. You need 1 bias for each filter.\n",
    "        filter_4 = tf.Variable(initializer((1, 4, embedding_size, 50)), name=\"weight4\")\n",
    "        bias_4 = tf.Variable(tf.zeros((1, 50)), name=\"bias4\")\n",
    "        conv2 = tf.nn.conv2d(\n",
    "            input=vectors2d,\n",
    "            filter=filter_4,\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding=\"VALID\"\n",
    "        )  # Shape = ?\n",
    "\n",
    "        A2 = tf.nn.leaky_relu(conv2 + bias_4)\n",
    "\n",
    "        # Create 50 filters with span of 5 words. You need 1 bias for each filter.\n",
    "        filter_5 = tf.Variable(initializer((1, 5, embedding_size, 50)), name=\"weight5\")\n",
    "        bias_5 = tf.Variable(tf.zeros((1, 50)), name=\"bias5\")\n",
    "        conv3 = tf.nn.conv2d(\n",
    "            input=vectors2d,\n",
    "            filter=filter_5,\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding=\"VALID\"\n",
    "        )  # Shape = ?\n",
    "\n",
    "        A3 = tf.nn.leaky_relu(conv3 + bias_5)\n",
    "\n",
    "        # Now extract the maximum activations for each of the filters. The shapes are listed alongside.\n",
    "        max_A1 = tf.squeeze(tf.nn.max_pool(A1,ksize=[1,1,98,1],strides=[1,1,1,1],padding='VALID'),[1,2])  # None x 50\n",
    "        max_A2 =tf.squeeze(tf.nn.max_pool(A2,ksize=[1,1,97,1],strides=[1,1,1,1],padding='VALID'),[1,2])  # None x 50\n",
    "        max_A3 = tf.squeeze(tf.nn.max_pool(A3,ksize=[1,1,96,1],strides=[1,1,1,1],padding='VALID'),[1,2]) # None x 50\n",
    "\n",
    "        concat = tf.concat([max_A1, max_A2, max_A3], axis=1)  # None x 150\n",
    "\n",
    "        # Initialize the weight and bias needed for softmax classifier.\n",
    "        self.softmax_weight = tf.Variable(initializer((150, 2)), name=\"W\",dtype=tf.float32)\n",
    "        self.softmax_bias = tf.Variable(tf.zeros(shape=[2]), name=\"b\",dtype=tf.float32)\n",
    "        logits = tf.matmul(concat, self.softmax_weight) + self.softmax_bias\n",
    "        # Write out the equation for computing the logits.\n",
    "        self.output = tf.nn.softmax(logits, axis=1)  # Shape = ?\n",
    "\n",
    "        # Compute the cross-entropy cost.\n",
    "        # You might either sum or take mean of all the costs across all the examples.\n",
    "        # It is your choice as the test case is on Stochastic Training.\n",
    "        entropy = tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(self.expected_output, 2), logits=logits)\n",
    "        self.cost = tf.reduce_mean(entropy)\n",
    "\n",
    "        correct_prediction = tf.equal(tf.reshape(tf.argmax(self.output, 1), [-1, 1]),\n",
    "                                      tf.cast(self.expected_output, dtype=tf.int64))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        self.train_op = optimizer.minimize(self.cost)\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "\n",
    "    def pad(self, data, pad_word, pad_length=100):\n",
    "        for datum in data:\n",
    "            datum.extend([pad_word] * (pad_length - len(datum)))\n",
    "        return data\n",
    "\n",
    "    def train(self, train_data, test_data, train_targets, test_targets, batch_size=1, epochs=1, verbose=False):\n",
    "        sess = self.session\n",
    "        self.pad(train_data, self.num_words)\n",
    "        self.pad(test_data, self.num_words)\n",
    "        print(\"Starting training...\")\n",
    "        for epoch in range(epochs):\n",
    "            cost_epoch = 0\n",
    "            c = 0\n",
    "            for datum, target in zip([train_data[i:i + batch_size] for i in range(0, len(train_data), batch_size)],\n",
    "                                     [train_targets[i:i + batch_size] for i in\n",
    "                                      range(0, len(train_targets), batch_size)]):\n",
    "                _, cost = sess.run([self.train_op, self.cost],\n",
    "                                   feed_dict={self.input: datum, self.expected_output: target})\n",
    "                cost_epoch += cost\n",
    "                c += 1\n",
    "                if c % 100 == 0 and verbose:\n",
    "                    print(\"\\t{} batches finished. Cost : {}\".format(c, cost_epoch / c))\n",
    "            print(\"Epoch {}: {}\".format(epoch, cost_epoch / len(train_data)))\n",
    "            print(\"\\tTrain accuracy: {}\".format(self.compute_accuracy(train_data, train_targets)))\n",
    "            print(\"\\tTest accuracy: {}\".format(self.compute_accuracy(test_data, test_targets)))\n",
    "        \n",
    "\n",
    "    def compute_accuracy(self, data, targets):\n",
    "        return self.session.run(self.accuracy, feed_dict={self.input: data, self.expected_output: targets})\n",
    "      \n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1512
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 61685,
     "status": "ok",
     "timestamp": 1529105457192,
     "user": {
      "displayName": "Prashant Pandey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "110318069328577762879"
     },
     "user_tz": 180
    },
    "id": "dfYIkXOkGmov",
    "outputId": "92b0383e-3e6b-4a12-8683-6e39286d8718"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\t100 batches finished. Cost : 0.692921392918\n",
      "\t200 batches finished. Cost : 0.69459349066\n",
      "\t300 batches finished. Cost : 0.695016728242\n",
      "\t400 batches finished. Cost : 0.695038382858\n",
      "\t500 batches finished. Cost : 0.693232012987\n",
      "\t600 batches finished. Cost : 0.692247983317\n",
      "\t700 batches finished. Cost : 0.692330603855\n",
      "\t800 batches finished. Cost : 0.687976080738\n",
      "\t900 batches finished. Cost : 0.684561155306\n",
      "\t1000 batches finished. Cost : 0.673103282005\n",
      "\t1100 batches finished. Cost : 0.664120773487\n",
      "\t1200 batches finished. Cost : 0.65733066816\n",
      "\t1300 batches finished. Cost : 0.65695838178\n",
      "\t1400 batches finished. Cost : 0.657977693046\n",
      "\t1500 batches finished. Cost : 0.654466114514\n",
      "\t1600 batches finished. Cost : 0.651420616158\n",
      "\t1700 batches finished. Cost : 0.650820456921\n",
      "\t1800 batches finished. Cost : 0.65132466364\n",
      "\t1900 batches finished. Cost : 0.650277696307\n",
      "\t2000 batches finished. Cost : 0.647244839464\n",
      "\t2100 batches finished. Cost : 0.642894521569\n",
      "\t2200 batches finished. Cost : 0.644193624597\n",
      "\t2300 batches finished. Cost : 0.640458194127\n",
      "\t2400 batches finished. Cost : 0.638717840151\n",
      "\t2500 batches finished. Cost : 0.636248194776\n",
      "\t2600 batches finished. Cost : 0.63466240807\n",
      "\t2700 batches finished. Cost : 0.633134302174\n",
      "\t2800 batches finished. Cost : 0.635907227958\n",
      "\t2900 batches finished. Cost : 0.632369569822\n",
      "\t3000 batches finished. Cost : 0.631557471717\n",
      "\t3100 batches finished. Cost : 0.630397655338\n",
      "\t3200 batches finished. Cost : 0.628197908554\n",
      "\t3300 batches finished. Cost : 0.628467252391\n",
      "\t3400 batches finished. Cost : 0.626152619832\n",
      "\t3500 batches finished. Cost : 0.62667701944\n",
      "\t3600 batches finished. Cost : 0.624941086048\n",
      "\t3700 batches finished. Cost : 0.623319454544\n",
      "\t3800 batches finished. Cost : 0.622756505487\n",
      "\t3900 batches finished. Cost : 0.62015989743\n",
      "\t4000 batches finished. Cost : 0.61682571347\n",
      "\t4100 batches finished. Cost : 0.615805965032\n",
      "\t4200 batches finished. Cost : 0.613346522652\n",
      "\t4300 batches finished. Cost : 0.609962923449\n",
      "\t4400 batches finished. Cost : 0.610294384063\n",
      "\t4500 batches finished. Cost : 0.609069744127\n",
      "\t4600 batches finished. Cost : 0.609528466889\n",
      "\t4700 batches finished. Cost : 0.608768019337\n",
      "\t4800 batches finished. Cost : 0.606916878385\n",
      "\t4900 batches finished. Cost : 0.604413389283\n",
      "\t5000 batches finished. Cost : 0.604509383955\n",
      "\t5100 batches finished. Cost : 0.602949771348\n",
      "\t5200 batches finished. Cost : 0.601748316132\n",
      "\t5300 batches finished. Cost : 0.600508345432\n",
      "\t5400 batches finished. Cost : 0.599324503187\n",
      "\t5500 batches finished. Cost : 0.598008038101\n",
      "\t5600 batches finished. Cost : 0.597085134804\n",
      "\t5700 batches finished. Cost : 0.595510007359\n",
      "\t5800 batches finished. Cost : 0.594375951129\n",
      "\t5900 batches finished. Cost : 0.59318779532\n",
      "\t6000 batches finished. Cost : 0.592798195819\n",
      "\t6100 batches finished. Cost : 0.592577717788\n",
      "\t6200 batches finished. Cost : 0.591251639051\n",
      "\t6300 batches finished. Cost : 0.591173994663\n",
      "\t6400 batches finished. Cost : 0.589626571603\n",
      "\t6500 batches finished. Cost : 0.588453625354\n",
      "\t6600 batches finished. Cost : 0.586582930301\n",
      "\t6700 batches finished. Cost : 0.585188382717\n",
      "\t6800 batches finished. Cost : 0.584333865033\n",
      "\t6900 batches finished. Cost : 0.583981918683\n",
      "\t7000 batches finished. Cost : 0.582811890702\n",
      "\t7100 batches finished. Cost : 0.581484058278\n",
      "\t7200 batches finished. Cost : 0.58052526948\n",
      "\t7300 batches finished. Cost : 0.580589930415\n",
      "\t7400 batches finished. Cost : 0.579671006402\n",
      "\t7500 batches finished. Cost : 0.578560322372\n",
      "\t7600 batches finished. Cost : 0.577817945144\n",
      "\t7700 batches finished. Cost : 0.577132987973\n",
      "\t7800 batches finished. Cost : 0.575841178018\n",
      "\t7900 batches finished. Cost : 0.575053566127\n",
      "\t8000 batches finished. Cost : 0.575663316636\n",
      "\t8100 batches finished. Cost : 0.57497548028\n",
      "\t8200 batches finished. Cost : 0.574744771542\n",
      "\t8300 batches finished. Cost : 0.572670773593\n",
      "\t8400 batches finished. Cost : 0.570655855511\n",
      "\t8500 batches finished. Cost : 0.570109953935\n",
      "Epoch 0: 0.571120104461\n",
      "\tTrain accuracy: 0.897057116032\n",
      "\tTest accuracy: 0.766526043415\n"
     ]
    }
   ],
   "source": [
    "c=CNN(len(vocab))\n",
    "c.train(train_data,test_data,train_targets,test_targets,epochs=1,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "T4Q8tG1csGG5"
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dhrJL02kGmow"
   },
   "source": [
    "The expected output for the above snippet is\n",
    "<pre>\n",
    "Starting training...\n",
    "\t100 batches finished. Cost : 0.697723334432\n",
    "\t200 batches finished. Cost : 0.69957424134\n",
    "\t300 batches finished. Cost : 0.697673715353\n",
    "\t400 batches finished. Cost : 0.692196451947\n",
    "\t500 batches finished. Cost : 0.693883402467\n",
    "    ...\n",
    "Epoch 0: 0.624233247656\n",
    "\tTrain accuracy: 0.828467607498\n",
    "\tTest accuracy: 0.736521303654   \n",
    "</pre>\n",
    "If you get any other output and you feel you are correct, you can proceed (However, I cannot think of any case where you can get a different output). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ozMFo7kGmow"
   },
   "source": [
    "### Effect of Batch Size on Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tybsvV02Gmox"
   },
   "source": [
    "Study the effects of changing batch size. Just run the various experiments and observe the results (Run it in non-verbose mode). No need to make any comments here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 60422,
     "status": "ok",
     "timestamp": 1529105518329,
     "user": {
      "displayName": "Prashant Pandey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "110318069328577762879"
     },
     "user_tz": 180
    },
    "id": "9JjNaWPtGmoy",
    "outputId": "c4f63175-7641-4097-daef-ae13a50cdfe9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 0: 0.571672957771\n",
      "\tTrain accuracy: 0.895181119442\n",
      "\tTest accuracy: 0.755743086338\n"
     ]
    }
   ],
   "source": [
    "c=CNN(len(vocab))\n",
    "c.train(train_data,test_data,train_targets,test_targets,epochs=1,verbose=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7624,
     "status": "ok",
     "timestamp": 1529105526034,
     "user": {
      "displayName": "Prashant Pandey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "110318069328577762879"
     },
     "user_tz": 180
    },
    "id": "Q1JOLED2vxhz",
    "outputId": "1d4a3f8b-a61c-4eb9-a903-d48ddd774782"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 0: 0.0592941020271\n",
      "\tTrain accuracy: 0.889201521873\n",
      "\tTest accuracy: 0.768401324749\n"
     ]
    }
   ],
   "source": [
    "c=CNN(len(vocab))\n",
    "c.train(train_data,test_data,train_targets,test_targets,epochs=1,verbose=False, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3048,
     "status": "ok",
     "timestamp": 1529105529205,
     "user": {
      "displayName": "Prashant Pandey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "110318069328577762879"
     },
     "user_tz": 180
    },
    "id": "0VGJ08qrv37A",
    "outputId": "2f9d2dfb-4f09-474d-fd83-f44b625f89e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 0: 0.00687814327013\n",
      "\tTrain accuracy: 0.755305409431\n",
      "\tTest accuracy: 0.67135488987\n"
     ]
    }
   ],
   "source": [
    "c=CNN(len(vocab))\n",
    "c.train(train_data,test_data,train_targets,test_targets,epochs=1,verbose=False, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KzOv1TG_Gmoz"
   },
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZVha1w_BGmo0"
   },
   "source": [
    "Add 2 functions - get_distance and get_most_similar to the CNN class (the big one). \n",
    "* get_distance(word1,word2) - should return the cosine distance between the 2 words.\n",
    "* get_most_similar(word) - should return top 10 most similar words to the word passed.\n",
    "\n",
    "Now, use the 2 functions to record the distances between a list of word-pairs as the training progresses. (One easy way to go about could be to save the embedding matrix in the hard-disk for every 5 updates.):\n",
    "* Study the distance between words of opposite sentiment as the training progresses. Ex: Good and Bad, Good and horrible, etc.\n",
    "* Study the distance between words of same sentiment. Ex: Good and Beautiful, Bad and Terrible, etc.\n",
    "* Study how the non-sentiment bearing words relate to each other. Ex: his, her, an, it, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "mamCZ7EZ2NQ1"
   },
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self, num_words, embedding_size=30):\n",
    "        self.num_words = num_words\n",
    "\n",
    "        # The batch of text documents. Let's assume that it is always padded to length 100.\n",
    "        # We could use [None,None], but we'll use [None,100] for simplicity.\n",
    "        self.input = tf.placeholder(shape=[None, 100], dtype=tf.int32)\n",
    "        self.expected_output = tf.placeholder(shape=[None, 1], dtype=tf.int32)\n",
    "\n",
    "        embedding_matrix = tf.Variable(initializer((num_words, embedding_size)), name=\"embeddings\")\n",
    "        # Add an additional row of zeros to denote padded words.\n",
    "        v2 = tf.Variable(tf.zeros([1, embedding_size]), dtype=tf.float32)\n",
    "\n",
    "        self.embedding_matrix = tf.concat([embedding_matrix, 0*v2], 0)\n",
    "\n",
    "        # Extract the vectors from the embedding matrix. The dimensions should be None x 100 x embedding_size.\n",
    "        # Use embedding lookup\n",
    "        vectors = tf.nn.embedding_lookup(self.embedding_matrix, self.input)  # None x 100 x embedding_size\n",
    "\n",
    "        # In order to use conv2d, we need vectors to be 4 dimensional.\n",
    "        # The convention is NHWC - None (Batch Size) x Height(Height of image) x Width(Width of image) x Channel(Depth - similar to RGB).\n",
    "        # For text, let's consider Height = 1, width = number of words, channel = embedding_size.\n",
    "        # Use expand-dims to modify.\n",
    "        vectors2d = tf.expand_dims(vectors, 1)  # None x 1 x 100 x embedding_size\n",
    "\n",
    "        # Create 50 filters with span of 3 words. You need 1 bias for each filter.\n",
    "        filter_tri = tf.Variable(initializer((1, 3, embedding_size, 50)), name=\"weight3\")\n",
    "        bias_tri = tf.Variable(tf.zeros((1, 50)), name=\"bias3\")\n",
    "        conv1 = tf.nn.conv2d(\n",
    "            input=vectors2d,\n",
    "            filter=filter_tri,\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding=\"VALID\"\n",
    "        )  # Shape = (None x 1 x 98 x 50)\n",
    "        A1 = tf.nn.leaky_relu(conv1 + bias_tri)\n",
    "   \n",
    "\n",
    "        # Create 50 filters with span of 4 words. You need 1 bias for each filter.\n",
    "        filter_4 = tf.Variable(initializer((1, 4, embedding_size, 50)), name=\"weight4\")\n",
    "        bias_4 = tf.Variable(tf.zeros((1, 50)), name=\"bias4\")\n",
    "        conv2 = tf.nn.conv2d(\n",
    "            input=vectors2d,\n",
    "            filter=filter_4,\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding=\"VALID\"\n",
    "        )  # Shape = ?\n",
    "\n",
    "        A2 = tf.nn.leaky_relu(conv2 + bias_4)\n",
    "\n",
    "        # Create 50 filters with span of 5 words. You need 1 bias for each filter.\n",
    "        filter_5 = tf.Variable(initializer((1, 5, embedding_size, 50)), name=\"weight5\")\n",
    "        bias_5 = tf.Variable(tf.zeros((1, 50)), name=\"bias5\")\n",
    "        conv3 = tf.nn.conv2d(\n",
    "            input=vectors2d,\n",
    "            filter=filter_5,\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding=\"VALID\"\n",
    "        )  # Shape = ?\n",
    "\n",
    "        A3 = tf.nn.leaky_relu(conv3 + bias_5)\n",
    "\n",
    "        # Now extract the maximum activations for each of the filters. The shapes are listed alongside.\n",
    "        max_A1 = tf.squeeze(tf.nn.max_pool(A1,ksize=[1,1,98,1],strides=[1,1,1,1],padding='VALID'),[1,2])  # None x 50\n",
    "        max_A2 =tf.squeeze(tf.nn.max_pool(A2,ksize=[1,1,97,1],strides=[1,1,1,1],padding='VALID'),[1,2])  # None x 50\n",
    "        max_A3 = tf.squeeze(tf.nn.max_pool(A3,ksize=[1,1,96,1],strides=[1,1,1,1],padding='VALID'),[1,2]) # None x 50\n",
    "\n",
    "        concat = tf.concat([max_A1, max_A2, max_A3], axis=1)  # None x 150\n",
    "\n",
    "        # Initialize the weight and bias needed for softmax classifier.\n",
    "        self.softmax_weight = tf.Variable(initializer((150, 2)), name=\"W\",dtype=tf.float32)\n",
    "        self.softmax_bias = tf.Variable(tf.zeros(shape=[2]), name=\"b\",dtype=tf.float32)\n",
    "        logits = tf.matmul(concat, self.softmax_weight) + self.softmax_bias\n",
    "        # Write out the equation for computing the logits.\n",
    "        self.output = tf.nn.softmax(logits, axis=1)  # Shape = ?\n",
    "\n",
    "        # Compute the cross-entropy cost.\n",
    "        # You might either sum or take mean of all the costs across all the examples.\n",
    "        # It is your choice as the test case is on Stochastic Training.\n",
    "        entropy = tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(self.expected_output, 2), logits=logits)\n",
    "        self.cost = tf.reduce_mean(entropy)\n",
    "\n",
    "        correct_prediction = tf.equal(tf.reshape(tf.argmax(self.output, 1), [-1, 1]),\n",
    "                                      tf.cast(self.expected_output, dtype=tf.int64))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        self.train_op = optimizer.minimize(self.cost)\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "\n",
    "    def pad(self, data, pad_word, pad_length=100):\n",
    "        for datum in data:\n",
    "            datum.extend([pad_word] * (pad_length - len(datum)))\n",
    "        return data\n",
    "\n",
    "    def train(self, train_data, test_data, train_targets, test_targets, batch_size=1, epochs=1, verbose=False, get_distance=False, similar_words=False):\n",
    "        sess = self.session\n",
    "        self.pad(train_data, self.num_words)\n",
    "        self.pad(test_data, self.num_words)\n",
    "        print(\"Starting training...\")\n",
    "        \n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "          \n",
    "            cost_epoch = 0\n",
    "            c = 0\n",
    "            for datum, target in zip([train_data[i:i + batch_size] for i in range(0, len(train_data), batch_size)],\n",
    "                                     [train_targets[i:i + batch_size] for i in\n",
    "                                      range(0, len(train_targets), batch_size)]):\n",
    "                _, cost = sess.run([self.train_op, self.cost],\n",
    "                                   feed_dict={self.input: datum, self.expected_output: target})\n",
    "                cost_epoch += cost\n",
    "                c += 1\n",
    "                if c % 100 == 0 and verbose:\n",
    "                    print(\"\\t{} batches finished. Cost : {}\".format(c, cost_epoch / c))\n",
    "            print(\"Epoch {}: {}\".format(epoch, cost_epoch / len(train_data)))\n",
    "            print(\"\\tTrain accuracy: {}\".format(self.compute_accuracy(train_data, train_targets)))\n",
    "            print(\"\\tTest accuracy: {}\".format(self.compute_accuracy(test_data, test_targets)))\n",
    "        if(get_distance==True):\n",
    "            distances = self.get_distance(\"good\",[\"outstanding\", \"marvelous\", \"awesome\"])\n",
    "            print (\"distances between 'good' and 'outsanding' : \", distances[0])\n",
    "            print (\"distances between 'good' and 'marvelous' : \", distances[1])\n",
    "            print (\"distances between 'good' and 'awesome' : \", distances[2])\n",
    "            distances = self.get_distance(\"bad\",[\"bore\", \"tiresome\", \"disaster\"])\n",
    "            print (\"distances between 'bad' and 'bore' : \", distances[0])\n",
    "            print (\"distances between 'bad' and 'tiresome' : \", distances[1])\n",
    "            print (\"distances between 'bad' and 'disaster' : \", distances[2])\n",
    "            distances = self.get_distance(\"bad\",[\"good\", \"outstanding\", \"bore\"])\n",
    "            print (\"distances between 'bad' and 'good' : \", distances[0])\n",
    "            print (\"distances between 'bad' and 'outstanding' : \", distances[1])\n",
    "            print (\"distances between 'bad' and 'bore' : \", distances[2])\n",
    "            distances = self.get_distance(\"his\",[\"good\", \"outstanding\", \"bad\"])\n",
    "            print (\"distances between 'his' and 'good' : \", distances[0])\n",
    "            print (\"distances between 'his' and 'outstanding' : \", distances[1])\n",
    "            print (\"distances between 'his' and 'bad' : \", distances[2])\n",
    "            distances = self.get_distance(\"his\",[\"her\", \"the\", \"it\"])\n",
    "            print (\"distances between 'his' and 'her' : \", distances[0])\n",
    "            print (\"distances between 'his' and 'the' : \", distances[1])\n",
    "            print (\"distances between 'his' and 'it' : \", distances[2])\n",
    "        \n",
    "        if(similar_words==True):\n",
    "            print (self.get_most_similar_word(\"good\")) \n",
    "            print (self.get_most_similar_word(\"outstanding\"))\n",
    "            print (self.get_most_similar_word(\"pointless\"))\n",
    "            print (self.get_most_similar_word(\"bore\"))\n",
    "            print (self.get_most_similar_word(\"bad\"))\n",
    "    \n",
    "    def get_distance(self, word1, word2):\n",
    "        em = self.session.run(self.embedding_matrix)\n",
    "        indexes1 = vocab[word1]\n",
    "        indexes2 = np.array([vocab[i] for i in word2])\n",
    "        dist1 = em[indexes1]\n",
    "        dist2 = [em[i] for i in indexes2]\n",
    "        k=np.reciprocal(norm(dist1)*(norm(dist2,axis=1)))\n",
    "        cos_similarity=(np.matmul(dist2,dist1))*(k)\n",
    "        return (cos_similarity)\n",
    "        \n",
    "    def get_most_similar_word(self, word):\n",
    "        print(\"\\n\\nmost similar wors for : \", word)\n",
    "        elems = [i for i in vocab.keys() if len(i) >= 3]\n",
    "        cos_sim=np.array(self.get_distance(word,elems))\n",
    "        z= (cos_sim.argsort()[::-1][:10])\n",
    "        for i in z:\n",
    "            print (elems[i])\n",
    "        \n",
    "\n",
    "    def compute_accuracy(self, data, targets):\n",
    "        return self.session.run(self.accuracy, feed_dict={self.input: data, self.expected_output: targets})\n",
    "      \n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1785
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 63348,
     "status": "ok",
     "timestamp": 1529106343356,
     "user": {
      "displayName": "Nikhil Dhirmalani",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "117732879546167846741"
     },
     "user_tz": 180
    },
    "id": "8Ar0QJlpxidK",
    "outputId": "20ff62d5-a2c2-4948-e4b7-274c664114d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\t100 batches finished. Cost : 0.69292139411\n",
      "\t200 batches finished. Cost : 0.694593482018\n",
      "\t300 batches finished. Cost : 0.695016701023\n",
      "\t400 batches finished. Cost : 0.695035839677\n",
      "\t500 batches finished. Cost : 0.693291908741\n",
      "\t600 batches finished. Cost : 0.692537637552\n",
      "\t700 batches finished. Cost : 0.692302601167\n",
      "\t800 batches finished. Cost : 0.688138718046\n",
      "\t900 batches finished. Cost : 0.684975091318\n",
      "\t1000 batches finished. Cost : 0.673650312752\n",
      "\t1100 batches finished. Cost : 0.665132350011\n",
      "\t1200 batches finished. Cost : 0.658742408901\n",
      "\t1300 batches finished. Cost : 0.657242710459\n",
      "\t1400 batches finished. Cost : 0.659336305525\n",
      "\t1500 batches finished. Cost : 0.656265250344\n",
      "\t1600 batches finished. Cost : 0.654087450743\n",
      "\t1700 batches finished. Cost : 0.652741341561\n",
      "\t1800 batches finished. Cost : 0.6533050329\n",
      "\t1900 batches finished. Cost : 0.651799985599\n",
      "\t2000 batches finished. Cost : 0.649399032458\n",
      "\t2100 batches finished. Cost : 0.645357313853\n",
      "\t2200 batches finished. Cost : 0.645808438823\n",
      "\t2300 batches finished. Cost : 0.642347100912\n",
      "\t2400 batches finished. Cost : 0.640546703115\n",
      "\t2500 batches finished. Cost : 0.63821529257\n",
      "\t2600 batches finished. Cost : 0.636304997329\n",
      "\t2700 batches finished. Cost : 0.635228063177\n",
      "\t2800 batches finished. Cost : 0.636887676136\n",
      "\t2900 batches finished. Cost : 0.63322297624\n",
      "\t3000 batches finished. Cost : 0.632424509863\n",
      "\t3100 batches finished. Cost : 0.631121806964\n",
      "\t3200 batches finished. Cost : 0.629105249074\n",
      "\t3300 batches finished. Cost : 0.62896311687\n",
      "\t3400 batches finished. Cost : 0.626922696579\n",
      "\t3500 batches finished. Cost : 0.62716859095\n",
      "\t3600 batches finished. Cost : 0.625290316042\n",
      "\t3700 batches finished. Cost : 0.623960087697\n",
      "\t3800 batches finished. Cost : 0.622921956493\n",
      "\t3900 batches finished. Cost : 0.62059033456\n",
      "\t4000 batches finished. Cost : 0.617131208816\n",
      "\t4100 batches finished. Cost : 0.615732897395\n",
      "\t4200 batches finished. Cost : 0.613492624524\n",
      "\t4300 batches finished. Cost : 0.610313237737\n",
      "\t4400 batches finished. Cost : 0.610856953145\n",
      "\t4500 batches finished. Cost : 0.609711523221\n",
      "\t4600 batches finished. Cost : 0.610301225048\n",
      "\t4700 batches finished. Cost : 0.60960911455\n",
      "\t4800 batches finished. Cost : 0.607581725919\n",
      "\t4900 batches finished. Cost : 0.605703000127\n",
      "\t5000 batches finished. Cost : 0.605714094101\n",
      "\t5100 batches finished. Cost : 0.604081863554\n",
      "\t5200 batches finished. Cost : 0.603145010734\n",
      "\t5300 batches finished. Cost : 0.602186690022\n",
      "\t5400 batches finished. Cost : 0.601264157344\n",
      "\t5500 batches finished. Cost : 0.59986046703\n",
      "\t5600 batches finished. Cost : 0.599018956921\n",
      "\t5700 batches finished. Cost : 0.597856159955\n",
      "\t5800 batches finished. Cost : 0.596124445777\n",
      "\t5900 batches finished. Cost : 0.594745064916\n",
      "\t6000 batches finished. Cost : 0.594486866832\n",
      "\t6100 batches finished. Cost : 0.594276168937\n",
      "\t6200 batches finished. Cost : 0.592741668146\n",
      "\t6300 batches finished. Cost : 0.592539441572\n",
      "\t6400 batches finished. Cost : 0.591195647852\n",
      "\t6500 batches finished. Cost : 0.589876534811\n",
      "\t6600 batches finished. Cost : 0.587668020718\n",
      "\t6700 batches finished. Cost : 0.586187500805\n",
      "\t6800 batches finished. Cost : 0.585486001557\n",
      "\t6900 batches finished. Cost : 0.585314193418\n",
      "\t7000 batches finished. Cost : 0.584520181537\n",
      "\t7100 batches finished. Cost : 0.583135028891\n",
      "\t7200 batches finished. Cost : 0.582000412368\n",
      "\t7300 batches finished. Cost : 0.581727889689\n",
      "\t7400 batches finished. Cost : 0.581144111733\n",
      "\t7500 batches finished. Cost : 0.58001423196\n",
      "\t7600 batches finished. Cost : 0.579083230286\n",
      "\t7700 batches finished. Cost : 0.57845338765\n",
      "\t7800 batches finished. Cost : 0.577618259946\n",
      "\t7900 batches finished. Cost : 0.576623286062\n",
      "\t8000 batches finished. Cost : 0.577411116592\n",
      "\t8100 batches finished. Cost : 0.576745037138\n",
      "\t8200 batches finished. Cost : 0.576490214304\n",
      "\t8300 batches finished. Cost : 0.574596154321\n",
      "\t8400 batches finished. Cost : 0.57266843576\n",
      "\t8500 batches finished. Cost : 0.572195797997\n",
      "Epoch 0: 0.573171856894\n",
      "\tTrain accuracy: 0.894477665424\n",
      "\tTest accuracy: 0.76371306181\n",
      "distances between 'good' and 'outsanding' :  0.7812242\n",
      "distances between 'good' and 'marvelous' :  0.54624295\n",
      "distances between 'good' and 'awesome' :  0.5953623\n",
      "distances between 'bad' and 'bore' :  0.7213952\n",
      "distances between 'bad' and 'tiresome' :  0.6378011\n",
      "distances between 'bad' and 'disaster' :  0.74368024\n",
      "distances between 'bad' and 'good' :  -0.4186115\n",
      "distances between 'bad' and 'outstanding' :  -0.62361896\n",
      "distances between 'bad' and 'bore' :  0.7213952\n",
      "distances between 'his' and 'good' :  0.51916885\n",
      "distances between 'his' and 'outstanding' :  0.5215573\n",
      "distances between 'his' and 'bad' :  -0.38066962\n",
      "distances between 'his' and 'her' :  0.37103778\n",
      "distances between 'his' and 'the' :  0.17956012\n",
      "distances between 'his' and 'it' :  0.29573607\n"
     ]
    }
   ],
   "source": [
    "c=CNN(len(vocab))\n",
    "c.train(train_data,test_data,train_targets,test_targets,epochs=1,verbose=True, get_distance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 2720
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 63005,
     "status": "ok",
     "timestamp": 1529106412249,
     "user": {
      "displayName": "Nikhil Dhirmalani",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "117732879546167846741"
     },
     "user_tz": 180
    },
    "id": "4I6UJMaSlpB-",
    "outputId": "e905713d-f776-4c3c-d60b-5dd905acbcc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\t100 batches finished. Cost : 0.692921392322\n",
      "\t200 batches finished. Cost : 0.69459348917\n",
      "\t300 batches finished. Cost : 0.695016726653\n",
      "\t400 batches finished. Cost : 0.695038380325\n",
      "\t500 batches finished. Cost : 0.693232011437\n",
      "\t600 batches finished. Cost : 0.692247983217\n",
      "\t700 batches finished. Cost : 0.692330602748\n",
      "\t800 batches finished. Cost : 0.687976078689\n",
      "\t900 batches finished. Cost : 0.684561153452\n",
      "\t1000 batches finished. Cost : 0.673103279248\n",
      "\t1100 batches finished. Cost : 0.66412077159\n",
      "\t1200 batches finished. Cost : 0.657330665812\n",
      "\t1300 batches finished. Cost : 0.65695837701\n",
      "\t1400 batches finished. Cost : 0.65797768932\n",
      "\t1500 batches finished. Cost : 0.654466109843\n",
      "\t1600 batches finished. Cost : 0.651420612627\n",
      "\t1700 batches finished. Cost : 0.650820453676\n",
      "\t1800 batches finished. Cost : 0.651324660655\n",
      "\t1900 batches finished. Cost : 0.650277691686\n",
      "\t2000 batches finished. Cost : 0.647244831026\n",
      "\t2100 batches finished. Cost : 0.642894513911\n",
      "\t2200 batches finished. Cost : 0.644193614328\n",
      "\t2300 batches finished. Cost : 0.640458202236\n",
      "\t2400 batches finished. Cost : 0.638717845423\n",
      "\t2500 batches finished. Cost : 0.636248199465\n",
      "\t2600 batches finished. Cost : 0.634662409438\n",
      "\t2700 batches finished. Cost : 0.633134306907\n",
      "\t2800 batches finished. Cost : 0.635907229654\n",
      "\t2900 batches finished. Cost : 0.632369571048\n",
      "\t3000 batches finished. Cost : 0.631557468665\n",
      "\t3100 batches finished. Cost : 0.630397650654\n",
      "\t3200 batches finished. Cost : 0.628197900764\n",
      "\t3300 batches finished. Cost : 0.628467245755\n",
      "\t3400 batches finished. Cost : 0.626152612867\n",
      "\t3500 batches finished. Cost : 0.626677017112\n",
      "\t3600 batches finished. Cost : 0.624941085078\n",
      "\t3700 batches finished. Cost : 0.623319351163\n",
      "\t3800 batches finished. Cost : 0.622758062854\n",
      "\t3900 batches finished. Cost : 0.620159697615\n",
      "\t4000 batches finished. Cost : 0.616828107103\n",
      "\t4100 batches finished. Cost : 0.615813332598\n",
      "\t4200 batches finished. Cost : 0.61336154257\n",
      "\t4300 batches finished. Cost : 0.609954580439\n",
      "\t4400 batches finished. Cost : 0.610293538698\n",
      "\t4500 batches finished. Cost : 0.609117533138\n",
      "\t4600 batches finished. Cost : 0.60955479087\n",
      "\t4700 batches finished. Cost : 0.608805244646\n",
      "\t4800 batches finished. Cost : 0.606952823767\n",
      "\t4900 batches finished. Cost : 0.6044287586\n",
      "\t5000 batches finished. Cost : 0.604506018915\n",
      "\t5100 batches finished. Cost : 0.602946281842\n",
      "\t5200 batches finished. Cost : 0.601716053293\n",
      "\t5300 batches finished. Cost : 0.6005994902\n",
      "\t5400 batches finished. Cost : 0.599456408668\n",
      "\t5500 batches finished. Cost : 0.59808381896\n",
      "\t5600 batches finished. Cost : 0.597161694982\n",
      "\t5700 batches finished. Cost : 0.595620738109\n",
      "\t5800 batches finished. Cost : 0.594483757947\n",
      "\t5900 batches finished. Cost : 0.593374642398\n",
      "\t6000 batches finished. Cost : 0.59291709972\n",
      "\t6100 batches finished. Cost : 0.592700171799\n",
      "\t6200 batches finished. Cost : 0.591324005182\n",
      "\t6300 batches finished. Cost : 0.591053399944\n",
      "\t6400 batches finished. Cost : 0.589456068213\n",
      "\t6500 batches finished. Cost : 0.588066284301\n",
      "\t6600 batches finished. Cost : 0.586220946735\n",
      "\t6700 batches finished. Cost : 0.584774305322\n",
      "\t6800 batches finished. Cost : 0.583784615738\n",
      "\t6900 batches finished. Cost : 0.583566168177\n",
      "\t7000 batches finished. Cost : 0.582532657338\n",
      "\t7100 batches finished. Cost : 0.58113231527\n",
      "\t7200 batches finished. Cost : 0.580153519777\n",
      "\t7300 batches finished. Cost : 0.580131971528\n",
      "\t7400 batches finished. Cost : 0.579201421768\n",
      "\t7500 batches finished. Cost : 0.578147772444\n",
      "\t7600 batches finished. Cost : 0.577178860504\n",
      "\t7700 batches finished. Cost : 0.576442528961\n",
      "\t7800 batches finished. Cost : 0.575208096574\n",
      "\t7900 batches finished. Cost : 0.574299615835\n",
      "\t8000 batches finished. Cost : 0.574870622244\n",
      "\t8100 batches finished. Cost : 0.574195875853\n",
      "\t8200 batches finished. Cost : 0.573878565673\n",
      "\t8300 batches finished. Cost : 0.571680337227\n",
      "\t8400 batches finished. Cost : 0.569601237898\n",
      "\t8500 batches finished. Cost : 0.569159898564\n",
      "Epoch 0: 0.570069035405\n",
      "\tTrain accuracy: 0.898229598999\n",
      "\tTest accuracy: 0.766057193279\n",
      "\n",
      "\n",
      "most similar wors for :  good\n",
      "good\n",
      "casts\n",
      "everyday\n",
      "viewed\n",
      "form\n",
      "world\n",
      "constructed\n",
      "elling\n",
      "flawed\n",
      "journey\n",
      "None\n",
      "\n",
      "\n",
      "most similar wors for :  outstanding\n",
      "outstanding\n",
      "taut\n",
      "subgenre\n",
      "imax\n",
      "griffiths\n",
      "dormer\n",
      "chilling\n",
      "moist\n",
      "finely\n",
      "timely\n",
      "None\n",
      "\n",
      "\n",
      "most similar wors for :  pointless\n",
      "pointless\n",
      "uninteresting\n",
      "stupid\n",
      "relentlessly\n",
      "flat\n",
      "boring\n",
      "bob's\n",
      "unconvincing\n",
      "unfocused\n",
      "tiresome\n",
      "None\n",
      "\n",
      "\n",
      "most similar wors for :  bore\n",
      "bore\n",
      "unfunny\n",
      "bland\n",
      "bears\n",
      "mediocre\n",
      "miss\n",
      "miserable\n",
      "uninspired\n",
      "purpose\n",
      "empty\n",
      "None\n",
      "\n",
      "\n",
      "most similar wors for :  bad\n",
      "bad\n",
      "script\n",
      "slummer\n",
      "jump\n",
      "length\n",
      "sedate\n",
      "click\n",
      "pretentious\n",
      "overheated\n",
      "starter\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "c=CNN(len(vocab))\n",
    "c.train(train_data,test_data,train_targets,test_targets,epochs=1,verbose=True, similar_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 5967
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 181255,
     "status": "ok",
     "timestamp": 1529106610024,
     "user": {
      "displayName": "Nikhil Dhirmalani",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "117732879546167846741"
     },
     "user_tz": 180
    },
    "id": "uvfO58tHHRUx",
    "outputId": "bc726e68-9239-4eda-8819-c682f4870ba9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\t100 batches finished. Cost : 0.692921392322\n",
      "\t200 batches finished. Cost : 0.694593454599\n",
      "\t300 batches finished. Cost : 0.695016626914\n",
      "\t400 batches finished. Cost : 0.695035860091\n",
      "\t500 batches finished. Cost : 0.693291987062\n",
      "\t600 batches finished. Cost : 0.692537785272\n",
      "\t700 batches finished. Cost : 0.692331592185\n",
      "\t800 batches finished. Cost : 0.688160484396\n",
      "\t900 batches finished. Cost : 0.685053563747\n",
      "\t1000 batches finished. Cost : 0.673603527457\n",
      "\t1100 batches finished. Cost : 0.665467264077\n",
      "\t1200 batches finished. Cost : 0.658732547659\n",
      "\t1300 batches finished. Cost : 0.657277223196\n",
      "\t1400 batches finished. Cost : 0.659507887502\n",
      "\t1500 batches finished. Cost : 0.655823797951\n",
      "\t1600 batches finished. Cost : 0.653874699965\n",
      "\t1700 batches finished. Cost : 0.652356962546\n",
      "\t1800 batches finished. Cost : 0.652956746177\n",
      "\t1900 batches finished. Cost : 0.651616413987\n",
      "\t2000 batches finished. Cost : 0.64894239826\n",
      "\t2100 batches finished. Cost : 0.64535826513\n",
      "\t2200 batches finished. Cost : 0.646104550757\n",
      "\t2300 batches finished. Cost : 0.642524915763\n",
      "\t2400 batches finished. Cost : 0.640925546941\n",
      "\t2500 batches finished. Cost : 0.638088110552\n",
      "\t2600 batches finished. Cost : 0.635908066216\n",
      "\t2700 batches finished. Cost : 0.634987925825\n",
      "\t2800 batches finished. Cost : 0.636861131804\n",
      "\t2900 batches finished. Cost : 0.633321717601\n",
      "\t3000 batches finished. Cost : 0.632488206174\n",
      "\t3100 batches finished. Cost : 0.631496368028\n",
      "\t3200 batches finished. Cost : 0.629236307031\n",
      "\t3300 batches finished. Cost : 0.629345845877\n",
      "\t3400 batches finished. Cost : 0.627691116016\n",
      "\t3500 batches finished. Cost : 0.62810392976\n",
      "\t3600 batches finished. Cost : 0.626258110605\n",
      "\t3700 batches finished. Cost : 0.624887143372\n",
      "\t3800 batches finished. Cost : 0.62430257852\n",
      "\t3900 batches finished. Cost : 0.621805045773\n",
      "\t4000 batches finished. Cost : 0.618701851175\n",
      "\t4100 batches finished. Cost : 0.616994058097\n",
      "\t4200 batches finished. Cost : 0.614557598584\n",
      "\t4300 batches finished. Cost : 0.611596758567\n",
      "\t4400 batches finished. Cost : 0.611961165358\n",
      "\t4500 batches finished. Cost : 0.610605519071\n",
      "\t4600 batches finished. Cost : 0.611095903024\n",
      "\t4700 batches finished. Cost : 0.610188633888\n",
      "\t4800 batches finished. Cost : 0.608425592091\n",
      "\t4900 batches finished. Cost : 0.60610872371\n",
      "\t5000 batches finished. Cost : 0.605816760778\n",
      "\t5100 batches finished. Cost : 0.604177796198\n",
      "\t5200 batches finished. Cost : 0.603205640312\n",
      "\t5300 batches finished. Cost : 0.602125343273\n",
      "\t5400 batches finished. Cost : 0.601012666302\n",
      "\t5500 batches finished. Cost : 0.599482650039\n",
      "\t5600 batches finished. Cost : 0.598781727234\n",
      "\t5700 batches finished. Cost : 0.597597246972\n",
      "\t5800 batches finished. Cost : 0.596303636654\n",
      "\t5900 batches finished. Cost : 0.594904399653\n",
      "\t6000 batches finished. Cost : 0.594595795224\n",
      "\t6100 batches finished. Cost : 0.594030070687\n",
      "\t6200 batches finished. Cost : 0.592587222781\n",
      "\t6300 batches finished. Cost : 0.592340054278\n",
      "\t6400 batches finished. Cost : 0.59100589245\n",
      "\t6500 batches finished. Cost : 0.589454563327\n",
      "\t6600 batches finished. Cost : 0.587364969636\n",
      "\t6700 batches finished. Cost : 0.586114743194\n",
      "\t6800 batches finished. Cost : 0.585370904771\n",
      "\t6900 batches finished. Cost : 0.585230171197\n",
      "\t7000 batches finished. Cost : 0.58437132465\n",
      "\t7100 batches finished. Cost : 0.583218445195\n",
      "\t7200 batches finished. Cost : 0.582101432481\n",
      "\t7300 batches finished. Cost : 0.581961527749\n",
      "\t7400 batches finished. Cost : 0.581471716161\n",
      "\t7500 batches finished. Cost : 0.58024715932\n",
      "\t7600 batches finished. Cost : 0.579331398471\n",
      "\t7700 batches finished. Cost : 0.578249770977\n",
      "\t7800 batches finished. Cost : 0.577382493736\n",
      "\t7900 batches finished. Cost : 0.576660514744\n",
      "\t8000 batches finished. Cost : 0.577351542282\n",
      "\t8100 batches finished. Cost : 0.576547411325\n",
      "\t8200 batches finished. Cost : 0.576311320436\n",
      "\t8300 batches finished. Cost : 0.574464291092\n",
      "\t8400 batches finished. Cost : 0.572350606084\n",
      "\t8500 batches finished. Cost : 0.571764133037\n",
      "Epoch 0: 0.572690932311\n",
      "\tTrain accuracy: 0.895298421383\n",
      "\tTest accuracy: 0.760900139809\n",
      "\t100 batches finished. Cost : 0.554516005581\n",
      "\t200 batches finished. Cost : 0.498672781563\n",
      "\t300 batches finished. Cost : 0.514343679004\n",
      "\t400 batches finished. Cost : 0.520380467565\n",
      "\t500 batches finished. Cost : 0.489903208241\n",
      "\t600 batches finished. Cost : 0.481543203164\n",
      "\t700 batches finished. Cost : 0.467092507044\n",
      "\t800 batches finished. Cost : 0.456456599389\n",
      "\t900 batches finished. Cost : 0.451029952994\n",
      "\t1000 batches finished. Cost : 0.442414976971\n",
      "\t1100 batches finished. Cost : 0.426435525405\n",
      "\t1200 batches finished. Cost : 0.420896692413\n",
      "\t1300 batches finished. Cost : 0.418611190491\n",
      "\t1400 batches finished. Cost : 0.409533049855\n",
      "\t1500 batches finished. Cost : 0.409199606091\n",
      "\t1600 batches finished. Cost : 0.40664248625\n",
      "\t1700 batches finished. Cost : 0.401254092267\n",
      "\t1800 batches finished. Cost : 0.396421585852\n",
      "\t1900 batches finished. Cost : 0.39148292098\n",
      "\t2000 batches finished. Cost : 0.385844197039\n",
      "\t2100 batches finished. Cost : 0.382345381252\n",
      "\t2200 batches finished. Cost : 0.383478616169\n",
      "\t2300 batches finished. Cost : 0.379241770812\n",
      "\t2400 batches finished. Cost : 0.37515711573\n",
      "\t2500 batches finished. Cost : 0.371589442741\n",
      "\t2600 batches finished. Cost : 0.366313731005\n",
      "\t2700 batches finished. Cost : 0.361651597774\n",
      "\t2800 batches finished. Cost : 0.364763761143\n",
      "\t2900 batches finished. Cost : 0.360971252885\n",
      "\t3000 batches finished. Cost : 0.358457213635\n",
      "\t3100 batches finished. Cost : 0.357689388208\n",
      "\t3200 batches finished. Cost : 0.353917630156\n",
      "\t3300 batches finished. Cost : 0.354524368283\n",
      "\t3400 batches finished. Cost : 0.350645586409\n",
      "\t3500 batches finished. Cost : 0.349882213643\n",
      "\t3600 batches finished. Cost : 0.348955908521\n",
      "\t3700 batches finished. Cost : 0.346756002332\n",
      "\t3800 batches finished. Cost : 0.345682406165\n",
      "\t3900 batches finished. Cost : 0.34253731644\n",
      "\t4000 batches finished. Cost : 0.338869149898\n",
      "\t4100 batches finished. Cost : 0.339145395332\n",
      "\t4200 batches finished. Cost : 0.336269410529\n",
      "\t4300 batches finished. Cost : 0.333942926426\n",
      "\t4400 batches finished. Cost : 0.3317311918\n",
      "\t4500 batches finished. Cost : 0.330110863152\n",
      "\t4600 batches finished. Cost : 0.331828723549\n",
      "\t4700 batches finished. Cost : 0.330475428716\n",
      "\t4800 batches finished. Cost : 0.328782589777\n",
      "\t4900 batches finished. Cost : 0.327787433767\n",
      "\t5000 batches finished. Cost : 0.326166939688\n",
      "\t5100 batches finished. Cost : 0.32329628461\n",
      "\t5200 batches finished. Cost : 0.321902927335\n",
      "\t5300 batches finished. Cost : 0.321243268214\n",
      "\t5400 batches finished. Cost : 0.319821316966\n",
      "\t5500 batches finished. Cost : 0.31906587015\n",
      "\t5600 batches finished. Cost : 0.317978236007\n",
      "\t5700 batches finished. Cost : 0.316087773771\n",
      "\t5800 batches finished. Cost : 0.315451213007\n",
      "\t5900 batches finished. Cost : 0.313616352392\n",
      "\t6000 batches finished. Cost : 0.313636525363\n",
      "\t6100 batches finished. Cost : 0.31289204984\n",
      "\t6200 batches finished. Cost : 0.312207245777\n",
      "\t6300 batches finished. Cost : 0.312290956548\n",
      "\t6400 batches finished. Cost : 0.310978038112\n",
      "\t6500 batches finished. Cost : 0.308442601211\n",
      "\t6600 batches finished. Cost : 0.30738085337\n",
      "\t6700 batches finished. Cost : 0.3061463798\n",
      "\t6800 batches finished. Cost : 0.304632435578\n",
      "\t6900 batches finished. Cost : 0.303448568779\n",
      "\t7000 batches finished. Cost : 0.304109163403\n",
      "\t7100 batches finished. Cost : 0.302312995525\n",
      "\t7200 batches finished. Cost : 0.301460543731\n",
      "\t7300 batches finished. Cost : 0.30109670197\n",
      "\t7400 batches finished. Cost : 0.300184108581\n",
      "\t7500 batches finished. Cost : 0.298663184143\n",
      "\t7600 batches finished. Cost : 0.29823762185\n",
      "\t7700 batches finished. Cost : 0.298352992823\n",
      "\t7800 batches finished. Cost : 0.298145150816\n",
      "\t7900 batches finished. Cost : 0.298584602499\n",
      "\t8000 batches finished. Cost : 0.298495413234\n",
      "\t8100 batches finished. Cost : 0.298269983442\n",
      "\t8200 batches finished. Cost : 0.298295671437\n",
      "\t8300 batches finished. Cost : 0.296595699783\n",
      "\t8400 batches finished. Cost : 0.294977276283\n",
      "\t8500 batches finished. Cost : 0.294703379825\n",
      "Epoch 1: 0.295047105402\n",
      "\tTrain accuracy: 0.97033649683\n",
      "\tTest accuracy: 0.772151887417\n",
      "\t100 batches finished. Cost : 0.238061731446\n",
      "\t200 batches finished. Cost : 0.226609079881\n",
      "\t300 batches finished. Cost : 0.224029015887\n",
      "\t400 batches finished. Cost : 0.228814957572\n",
      "\t500 batches finished. Cost : 0.209060819838\n",
      "\t600 batches finished. Cost : 0.204808194124\n",
      "\t700 batches finished. Cost : 0.202224471109\n",
      "\t800 batches finished. Cost : 0.207446029492\n",
      "\t900 batches finished. Cost : 0.200481125166\n",
      "\t1000 batches finished. Cost : 0.198042942059\n",
      "\t1100 batches finished. Cost : 0.188815460361\n",
      "\t1200 batches finished. Cost : 0.184740207006\n",
      "\t1300 batches finished. Cost : 0.183829212541\n",
      "\t1400 batches finished. Cost : 0.17949782373\n",
      "\t1500 batches finished. Cost : 0.18003798648\n",
      "\t1600 batches finished. Cost : 0.178504617936\n",
      "\t1700 batches finished. Cost : 0.174113714117\n",
      "\t1800 batches finished. Cost : 0.169267330996\n",
      "\t1900 batches finished. Cost : 0.168905013725\n",
      "\t2000 batches finished. Cost : 0.164998562774\n",
      "\t2100 batches finished. Cost : 0.163332836413\n",
      "\t2200 batches finished. Cost : 0.162291454073\n",
      "\t2300 batches finished. Cost : 0.161180480912\n",
      "\t2400 batches finished. Cost : 0.159132758062\n",
      "\t2500 batches finished. Cost : 0.156633822951\n",
      "\t2600 batches finished. Cost : 0.153994350939\n",
      "\t2700 batches finished. Cost : 0.150254908723\n",
      "\t2800 batches finished. Cost : 0.153091215355\n",
      "\t2900 batches finished. Cost : 0.15095949006\n",
      "\t3000 batches finished. Cost : 0.149148889008\n",
      "\t3100 batches finished. Cost : 0.147550882476\n",
      "\t3200 batches finished. Cost : 0.146369749875\n",
      "\t3300 batches finished. Cost : 0.145476091239\n",
      "\t3400 batches finished. Cost : 0.142992115628\n",
      "\t3500 batches finished. Cost : 0.141561443707\n",
      "\t3600 batches finished. Cost : 0.139990638169\n",
      "\t3700 batches finished. Cost : 0.138797430342\n",
      "\t3800 batches finished. Cost : 0.136881107836\n",
      "\t3900 batches finished. Cost : 0.134190659976\n",
      "\t4000 batches finished. Cost : 0.131410252072\n",
      "\t4100 batches finished. Cost : 0.131947089505\n",
      "\t4200 batches finished. Cost : 0.130645522303\n",
      "\t4300 batches finished. Cost : 0.129787495728\n",
      "\t4400 batches finished. Cost : 0.12812903762\n",
      "\t4500 batches finished. Cost : 0.126864353277\n",
      "\t4600 batches finished. Cost : 0.127422390214\n",
      "\t4700 batches finished. Cost : 0.126122074988\n",
      "\t4800 batches finished. Cost : 0.124858796142\n",
      "\t4900 batches finished. Cost : 0.124853077723\n",
      "\t5000 batches finished. Cost : 0.123863349764\n",
      "\t5100 batches finished. Cost : 0.122210012204\n",
      "\t5200 batches finished. Cost : 0.120782004923\n",
      "\t5300 batches finished. Cost : 0.120762987758\n",
      "\t5400 batches finished. Cost : 0.119866002767\n",
      "\t5500 batches finished. Cost : 0.119618617102\n",
      "\t5600 batches finished. Cost : 0.118790843099\n",
      "\t5700 batches finished. Cost : 0.117260595361\n",
      "\t5800 batches finished. Cost : 0.115853923164\n",
      "\t5900 batches finished. Cost : 0.114662514352\n",
      "\t6000 batches finished. Cost : 0.115000942901\n",
      "\t6100 batches finished. Cost : 0.11362086789\n",
      "\t6200 batches finished. Cost : 0.112791983706\n",
      "\t6300 batches finished. Cost : 0.112938122325\n",
      "\t6400 batches finished. Cost : 0.11164645646\n",
      "\t6500 batches finished. Cost : 0.11046694671\n",
      "\t6600 batches finished. Cost : 0.109301086585\n",
      "\t6700 batches finished. Cost : 0.108769581362\n",
      "\t6800 batches finished. Cost : 0.107918093408\n",
      "\t6900 batches finished. Cost : 0.107031914844\n",
      "\t7000 batches finished. Cost : 0.106595343361\n",
      "\t7100 batches finished. Cost : 0.105432225216\n",
      "\t7200 batches finished. Cost : 0.105043283438\n",
      "\t7300 batches finished. Cost : 0.104480275325\n",
      "\t7400 batches finished. Cost : 0.103490230466\n",
      "\t7500 batches finished. Cost : 0.102655255272\n",
      "\t7600 batches finished. Cost : 0.103264302309\n",
      "\t7700 batches finished. Cost : 0.10309784896\n",
      "\t7800 batches finished. Cost : 0.102689830122\n",
      "\t7900 batches finished. Cost : 0.10284757715\n",
      "\t8000 batches finished. Cost : 0.102115077715\n",
      "\t8100 batches finished. Cost : 0.102030102027\n",
      "\t8200 batches finished. Cost : 0.101726765042\n",
      "\t8300 batches finished. Cost : 0.101071060354\n",
      "\t8400 batches finished. Cost : 0.100414081075\n",
      "\t8500 batches finished. Cost : 0.0997440000803\n",
      "Epoch 2: 0.0997647621393\n",
      "\tTrain accuracy: 0.992613434792\n",
      "\tTest accuracy: 0.756211936474\n",
      "distances between 'good' and 'outsanding' :  0.6243494\n",
      "distances between 'good' and 'marvelous' :  0.36796132\n",
      "distances between 'good' and 'awesome' :  0.344117\n",
      "distances between 'bad' and 'bore' :  0.5947187\n",
      "distances between 'bad' and 'tiresome' :  0.57971513\n",
      "distances between 'bad' and 'disaster' :  0.7317054\n",
      "distances between 'bad' and 'good' :  -0.383338\n",
      "distances between 'bad' and 'outstanding' :  -0.31684396\n",
      "distances between 'bad' and 'bore' :  0.59471875\n",
      "distances between 'his' and 'good' :  0.48593968\n",
      "distances between 'his' and 'outstanding' :  0.4625858\n",
      "distances between 'his' and 'bad' :  -0.5205904\n",
      "distances between 'his' and 'her' :  0.09834428\n",
      "distances between 'his' and 'the' :  0.13134694\n",
      "distances between 'his' and 'it' :  0.16171248\n",
      "\n",
      "\n",
      "most similar wors for :  good\n",
      "good\n",
      "sneaks\n",
      "stylish\n",
      "unpredictable\n",
      "duke\n",
      "fiery\n",
      "lagaan\n",
      "almodvar\n",
      "lux\n",
      "prickly\n",
      "None\n",
      "\n",
      "\n",
      "most similar wors for :  outstanding\n",
      "outstanding\n",
      "gently\n",
      "satisfyingly\n",
      "imagery\n",
      "abbass's\n",
      "satisfies\n",
      "concentration\n",
      "angela\n",
      "vividly\n",
      "accurate\n",
      "None\n",
      "\n",
      "\n",
      "most similar wors for :  pointless\n",
      "pointless\n",
      "unimaginative\n",
      "thrown\n",
      "bore\n",
      "clichs\n",
      "routine\n",
      "badly\n",
      "overreaches\n",
      "bicycle\n",
      "handful\n",
      "None\n",
      "\n",
      "\n",
      "most similar wors for :  bore\n",
      "bore\n",
      "fewer\n",
      "uninspired\n",
      "antique\n",
      "cable\n",
      "mediocre\n",
      "propaganda\n",
      "badly\n",
      "suit\n",
      "routine\n",
      "None\n",
      "\n",
      "\n",
      "most similar wors for :  bad\n",
      "bad\n",
      "didn't\n",
      "video\n",
      "hippest\n",
      "club\n",
      "gallic\n",
      "hyphenate\n",
      "eludes\n",
      "muddled\n",
      "joyless\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "c=CNN(len(vocab))\n",
    "c.train(train_data,test_data,train_targets,test_targets,epochs=3,verbose=True, get_distance=True, similar_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "zw4bN713Gmo0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMOB3iO5ag_X"
   },
   "source": [
    "### Learnings:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bj0s5x-FarR3"
   },
   "source": [
    "List out the observations and conclusions you made from the various experiments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "llj9565NIZpQ"
   },
   "source": [
    "\n",
    "\n",
    "1.   The words that share same tones tend to be closer to each other(that means the cosine distance between them is near to 1). The cos distance  between 'good' and 'outstanding' is ~0.6 and with 'awesome' is ~0.5, which depicts the similarity between their meanings. Same trend can be observed between the words carrying negative tones (~0.7 between 'bad' and 'disaster').\n",
    "\n",
    "2.    Words carying opposite tones have cosine distances of relatively larger magnitude. Cosine distance between opposite points in a space is close to -1: Distance between 'bad' and 'outsanding' is ~(-0.6). Similar trend can be seen in other words ( although mild in some cases due to lack of training examples).\n",
    "\n",
    "3.   Neutral words also displays expected behavior. The cosine distance between two neutral words is around 0 : Distance between 'his' and 'the' :  -0.08498761. The distance increases when we compare it to a word with high magnitude of 'tone'. The cosine distance comes around half the maximum distance as excpected by symmetry : Distance between 'his'(a neutral word) and 'good'(word with positive tone) :  0.5924598\n",
    "                          \n",
    "4. As we increase the number of epochs and make it 3 we feel that the model overfits the data. It simply memorizes the data and we get good training accuracy around 99 and there is not much difference in test accuracy, Moreover it decreases as compared to small epochs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "jsG33AL52jQD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Copy of A3.ipynb",
   "provenance": [
    {
     "file_id": "1Vg_mOVrg8tlz_ihKzNbZynKWVp4SmBZx",
     "timestamp": 1528300859595
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
